{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow & Keras - Basics of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most importantly... resources\n",
    "\n",
    "https://www.tensorflow.org/api_docs\n",
    "\n",
    "https://keras.io/\n",
    "\n",
    "https://www.tensorflow.org/tutorials/\n",
    "\n",
    "https://www.google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF overview\n",
    "\n",
    "* #### \"End-to-end machine learning platform\" \n",
    "\n",
    "    - Not the only one! Check out PyTorch, Theano, Cognitive Toolkit.\n",
    "   \n",
    "* #### Integrates with high-level APIs like Keras\n",
    "* #### Plays nice with Pandas\n",
    "* #### Makes deep learning *fast* and *easy* *\n",
    "    *<sup>\"easy\"</sup>\n",
    "\n",
    "## Tasks for TensorFlow:\n",
    "\n",
    "* #### Regression\n",
    "    - Predict house prices\n",
    "    - Predict drug metabolic rates\n",
    "    - Predict stock trends *\n",
    "    \n",
    "    *<sup>this is super hard</sup>\n",
    "    \n",
    "    \n",
    "\n",
    "* #### Classification\n",
    "    - Cat or dog?\n",
    "    - Malignant or benign cancer from images\n",
    "    ![](media/dr.png)\n",
    "    <span style=\"font-size:0.75em;\">Google AI Blog: Diabetic Retinopathy</span>\n",
    "\n",
    "\n",
    "\n",
    "* #### Dimensionality reduction\n",
    "    - Visualize high-dimensional data in 2 or 3-D space\n",
    "    - Compress representations for successive ML\n",
    "\n",
    "\n",
    "\n",
    "* #### Generative models\n",
    "    - Create new molecules with desirable properties\n",
    "    - Artificially enhance image resolution\n",
    "    ![](media/molecular_gan.png)\n",
    "    <span style=\"font-size:0.75em;\">Kadurin et al., 2017</span>\n",
    "\n",
    "\n",
    "* #### Reinforcement learning\n",
    "    - Can't beat your friends at chess? Make your computer do it\n",
    "\n",
    "\n",
    "\n",
    "* #### Much more...\n",
    "    - Generic math\n",
    "    - Probabilistic programming with TFP\n",
    "    - Automatic differentiation\n",
    "    - ...\n",
    "\n",
    "\n",
    "## Let's Regress\n",
    "\n",
    "### Imports!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name a more iconic duo, I'll wait\n",
    "\n",
    "#### New imports -- TF and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check our versions for good measure -- these programs may have very different behavior version-to-version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading in housing data as with SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7237550310</td>\n",
       "      <td>20140512T000000</td>\n",
       "      <td>1225000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.50</td>\n",
       "      <td>5420</td>\n",
       "      <td>101930</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>3890</td>\n",
       "      <td>1530</td>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "      <td>98053</td>\n",
       "      <td>47.6561</td>\n",
       "      <td>-122.005</td>\n",
       "      <td>4760</td>\n",
       "      <td>101930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1321400060</td>\n",
       "      <td>20140627T000000</td>\n",
       "      <td>257500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1715</td>\n",
       "      <td>6819</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1715</td>\n",
       "      <td>0</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>98003</td>\n",
       "      <td>47.3097</td>\n",
       "      <td>-122.327</td>\n",
       "      <td>2238</td>\n",
       "      <td>6819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2008000270</td>\n",
       "      <td>20150115T000000</td>\n",
       "      <td>291850.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1060</td>\n",
       "      <td>9711</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1060</td>\n",
       "      <td>0</td>\n",
       "      <td>1963</td>\n",
       "      <td>0</td>\n",
       "      <td>98198</td>\n",
       "      <td>47.4095</td>\n",
       "      <td>-122.315</td>\n",
       "      <td>1650</td>\n",
       "      <td>9711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2414600126</td>\n",
       "      <td>20150415T000000</td>\n",
       "      <td>229500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1780</td>\n",
       "      <td>7470</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>730</td>\n",
       "      <td>1960</td>\n",
       "      <td>0</td>\n",
       "      <td>98146</td>\n",
       "      <td>47.5123</td>\n",
       "      <td>-122.337</td>\n",
       "      <td>1780</td>\n",
       "      <td>8113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3793500160</td>\n",
       "      <td>20150312T000000</td>\n",
       "      <td>323000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1890</td>\n",
       "      <td>6560</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1890</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>98038</td>\n",
       "      <td>47.3684</td>\n",
       "      <td>-122.031</td>\n",
       "      <td>2390</td>\n",
       "      <td>7570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1736800520</td>\n",
       "      <td>20150403T000000</td>\n",
       "      <td>662500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3560</td>\n",
       "      <td>9796</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1860</td>\n",
       "      <td>1700</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98007</td>\n",
       "      <td>47.6007</td>\n",
       "      <td>-122.145</td>\n",
       "      <td>2210</td>\n",
       "      <td>8925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9212900260</td>\n",
       "      <td>20140527T000000</td>\n",
       "      <td>468000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1160</td>\n",
       "      <td>6000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>860</td>\n",
       "      <td>300</td>\n",
       "      <td>1942</td>\n",
       "      <td>0</td>\n",
       "      <td>98115</td>\n",
       "      <td>47.6900</td>\n",
       "      <td>-122.292</td>\n",
       "      <td>1330</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>114101516</td>\n",
       "      <td>20140528T000000</td>\n",
       "      <td>310000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1430</td>\n",
       "      <td>19901</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1430</td>\n",
       "      <td>0</td>\n",
       "      <td>1927</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7558</td>\n",
       "      <td>-122.229</td>\n",
       "      <td>1780</td>\n",
       "      <td>12697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6054650070</td>\n",
       "      <td>20141007T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1370</td>\n",
       "      <td>9680</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1370</td>\n",
       "      <td>0</td>\n",
       "      <td>1977</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6127</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1370</td>\n",
       "      <td>10208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1175000570</td>\n",
       "      <td>20150312T000000</td>\n",
       "      <td>530000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1810</td>\n",
       "      <td>4850</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1810</td>\n",
       "      <td>0</td>\n",
       "      <td>1900</td>\n",
       "      <td>0</td>\n",
       "      <td>98107</td>\n",
       "      <td>47.6700</td>\n",
       "      <td>-122.394</td>\n",
       "      <td>1360</td>\n",
       "      <td>4850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9297300055</td>\n",
       "      <td>20150124T000000</td>\n",
       "      <td>650000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2950</td>\n",
       "      <td>5000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>1980</td>\n",
       "      <td>970</td>\n",
       "      <td>1979</td>\n",
       "      <td>0</td>\n",
       "      <td>98126</td>\n",
       "      <td>47.5714</td>\n",
       "      <td>-122.375</td>\n",
       "      <td>2140</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1875500060</td>\n",
       "      <td>20140731T000000</td>\n",
       "      <td>395000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1890</td>\n",
       "      <td>14040</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1890</td>\n",
       "      <td>0</td>\n",
       "      <td>1994</td>\n",
       "      <td>0</td>\n",
       "      <td>98019</td>\n",
       "      <td>47.7277</td>\n",
       "      <td>-121.962</td>\n",
       "      <td>1890</td>\n",
       "      <td>14018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6865200140</td>\n",
       "      <td>20140529T000000</td>\n",
       "      <td>485000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1600</td>\n",
       "      <td>4300</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>1916</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6648</td>\n",
       "      <td>-122.343</td>\n",
       "      <td>1610</td>\n",
       "      <td>4300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>16000397</td>\n",
       "      <td>20141205T000000</td>\n",
       "      <td>189000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1200</td>\n",
       "      <td>9850</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1200</td>\n",
       "      <td>0</td>\n",
       "      <td>1921</td>\n",
       "      <td>0</td>\n",
       "      <td>98002</td>\n",
       "      <td>47.3089</td>\n",
       "      <td>-122.210</td>\n",
       "      <td>1060</td>\n",
       "      <td>5095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7983200060</td>\n",
       "      <td>20150424T000000</td>\n",
       "      <td>230000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1250</td>\n",
       "      <td>9774</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1250</td>\n",
       "      <td>0</td>\n",
       "      <td>1969</td>\n",
       "      <td>0</td>\n",
       "      <td>98003</td>\n",
       "      <td>47.3343</td>\n",
       "      <td>-122.306</td>\n",
       "      <td>1280</td>\n",
       "      <td>8850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6300500875</td>\n",
       "      <td>20140514T000000</td>\n",
       "      <td>385000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1620</td>\n",
       "      <td>4980</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>860</td>\n",
       "      <td>760</td>\n",
       "      <td>1947</td>\n",
       "      <td>0</td>\n",
       "      <td>98133</td>\n",
       "      <td>47.7025</td>\n",
       "      <td>-122.341</td>\n",
       "      <td>1400</td>\n",
       "      <td>4980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2524049179</td>\n",
       "      <td>20140826T000000</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3050</td>\n",
       "      <td>44867</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2330</td>\n",
       "      <td>720</td>\n",
       "      <td>1968</td>\n",
       "      <td>0</td>\n",
       "      <td>98040</td>\n",
       "      <td>47.5316</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>4110</td>\n",
       "      <td>20336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7137970340</td>\n",
       "      <td>20140703T000000</td>\n",
       "      <td>285000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2270</td>\n",
       "      <td>6300</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2270</td>\n",
       "      <td>0</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>98092</td>\n",
       "      <td>47.3266</td>\n",
       "      <td>-122.169</td>\n",
       "      <td>2240</td>\n",
       "      <td>7005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8091400200</td>\n",
       "      <td>20140516T000000</td>\n",
       "      <td>252700.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1070</td>\n",
       "      <td>9643</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1070</td>\n",
       "      <td>0</td>\n",
       "      <td>1985</td>\n",
       "      <td>0</td>\n",
       "      <td>98030</td>\n",
       "      <td>47.3533</td>\n",
       "      <td>-122.166</td>\n",
       "      <td>1220</td>\n",
       "      <td>8386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3814700200</td>\n",
       "      <td>20141120T000000</td>\n",
       "      <td>329000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2450</td>\n",
       "      <td>6500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2450</td>\n",
       "      <td>0</td>\n",
       "      <td>1985</td>\n",
       "      <td>0</td>\n",
       "      <td>98030</td>\n",
       "      <td>47.3739</td>\n",
       "      <td>-122.172</td>\n",
       "      <td>2200</td>\n",
       "      <td>6865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1202000200</td>\n",
       "      <td>20141103T000000</td>\n",
       "      <td>233000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1710</td>\n",
       "      <td>4697</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>1710</td>\n",
       "      <td>0</td>\n",
       "      <td>1941</td>\n",
       "      <td>0</td>\n",
       "      <td>98002</td>\n",
       "      <td>47.3048</td>\n",
       "      <td>-122.218</td>\n",
       "      <td>1030</td>\n",
       "      <td>4705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1794500383</td>\n",
       "      <td>20140626T000000</td>\n",
       "      <td>937000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2450</td>\n",
       "      <td>2691</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1750</td>\n",
       "      <td>700</td>\n",
       "      <td>1915</td>\n",
       "      <td>0</td>\n",
       "      <td>98119</td>\n",
       "      <td>47.6386</td>\n",
       "      <td>-122.360</td>\n",
       "      <td>1760</td>\n",
       "      <td>3573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3303700376</td>\n",
       "      <td>20141201T000000</td>\n",
       "      <td>667000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1400</td>\n",
       "      <td>1581</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1400</td>\n",
       "      <td>0</td>\n",
       "      <td>1909</td>\n",
       "      <td>0</td>\n",
       "      <td>98112</td>\n",
       "      <td>47.6221</td>\n",
       "      <td>-122.314</td>\n",
       "      <td>1860</td>\n",
       "      <td>3861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5101402488</td>\n",
       "      <td>20140624T000000</td>\n",
       "      <td>438000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1520</td>\n",
       "      <td>6380</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>790</td>\n",
       "      <td>730</td>\n",
       "      <td>1948</td>\n",
       "      <td>0</td>\n",
       "      <td>98115</td>\n",
       "      <td>47.6950</td>\n",
       "      <td>-122.304</td>\n",
       "      <td>1520</td>\n",
       "      <td>6235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1873100390</td>\n",
       "      <td>20150302T000000</td>\n",
       "      <td>719000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2570</td>\n",
       "      <td>7173</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2570</td>\n",
       "      <td>0</td>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>98052</td>\n",
       "      <td>47.7073</td>\n",
       "      <td>-122.110</td>\n",
       "      <td>2630</td>\n",
       "      <td>6026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21583</th>\n",
       "      <td>2025049203</td>\n",
       "      <td>20140610T000000</td>\n",
       "      <td>399950.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>710</td>\n",
       "      <td>1157</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>710</td>\n",
       "      <td>0</td>\n",
       "      <td>1943</td>\n",
       "      <td>0</td>\n",
       "      <td>98102</td>\n",
       "      <td>47.6413</td>\n",
       "      <td>-122.329</td>\n",
       "      <td>1370</td>\n",
       "      <td>1173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21584</th>\n",
       "      <td>952006823</td>\n",
       "      <td>20141202T000000</td>\n",
       "      <td>380000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1260</td>\n",
       "      <td>900</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>940</td>\n",
       "      <td>320</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>98116</td>\n",
       "      <td>47.5621</td>\n",
       "      <td>-122.384</td>\n",
       "      <td>1310</td>\n",
       "      <td>1415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21585</th>\n",
       "      <td>3832050760</td>\n",
       "      <td>20140828T000000</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1870</td>\n",
       "      <td>5000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1870</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98042</td>\n",
       "      <td>47.3339</td>\n",
       "      <td>-122.055</td>\n",
       "      <td>2170</td>\n",
       "      <td>5399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21586</th>\n",
       "      <td>2767604724</td>\n",
       "      <td>20141015T000000</td>\n",
       "      <td>505000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1430</td>\n",
       "      <td>1201</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1430</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98107</td>\n",
       "      <td>47.6707</td>\n",
       "      <td>-122.381</td>\n",
       "      <td>1430</td>\n",
       "      <td>1249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21587</th>\n",
       "      <td>6632300207</td>\n",
       "      <td>20150305T000000</td>\n",
       "      <td>385000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1520</td>\n",
       "      <td>1488</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1520</td>\n",
       "      <td>0</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7337</td>\n",
       "      <td>-122.309</td>\n",
       "      <td>1520</td>\n",
       "      <td>1497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21588</th>\n",
       "      <td>2767600688</td>\n",
       "      <td>20141113T000000</td>\n",
       "      <td>414500.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1210</td>\n",
       "      <td>1278</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1020</td>\n",
       "      <td>190</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>98117</td>\n",
       "      <td>47.6756</td>\n",
       "      <td>-122.375</td>\n",
       "      <td>1210</td>\n",
       "      <td>1118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21589</th>\n",
       "      <td>7570050450</td>\n",
       "      <td>20140910T000000</td>\n",
       "      <td>347500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2540</td>\n",
       "      <td>4760</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2540</td>\n",
       "      <td>0</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>98038</td>\n",
       "      <td>47.3452</td>\n",
       "      <td>-122.022</td>\n",
       "      <td>2540</td>\n",
       "      <td>4571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21590</th>\n",
       "      <td>7430200100</td>\n",
       "      <td>20140514T000000</td>\n",
       "      <td>1222500.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.50</td>\n",
       "      <td>4910</td>\n",
       "      <td>9444</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>3110</td>\n",
       "      <td>1800</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6502</td>\n",
       "      <td>-122.066</td>\n",
       "      <td>4560</td>\n",
       "      <td>11063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21591</th>\n",
       "      <td>4140940150</td>\n",
       "      <td>20141002T000000</td>\n",
       "      <td>572000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2770</td>\n",
       "      <td>3852</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2770</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5001</td>\n",
       "      <td>-122.232</td>\n",
       "      <td>1810</td>\n",
       "      <td>5641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21592</th>\n",
       "      <td>1931300412</td>\n",
       "      <td>20150416T000000</td>\n",
       "      <td>475000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1190</td>\n",
       "      <td>1200</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1190</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6542</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1180</td>\n",
       "      <td>1224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21593</th>\n",
       "      <td>8672200110</td>\n",
       "      <td>20150317T000000</td>\n",
       "      <td>1088000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4170</td>\n",
       "      <td>8142</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>4170</td>\n",
       "      <td>0</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>98056</td>\n",
       "      <td>47.5354</td>\n",
       "      <td>-122.181</td>\n",
       "      <td>3030</td>\n",
       "      <td>7980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21594</th>\n",
       "      <td>5087900040</td>\n",
       "      <td>20141017T000000</td>\n",
       "      <td>350000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2500</td>\n",
       "      <td>5995</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98042</td>\n",
       "      <td>47.3749</td>\n",
       "      <td>-122.107</td>\n",
       "      <td>2530</td>\n",
       "      <td>5988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21595</th>\n",
       "      <td>1972201967</td>\n",
       "      <td>20141031T000000</td>\n",
       "      <td>520000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1530</td>\n",
       "      <td>981</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1480</td>\n",
       "      <td>50</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6533</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1530</td>\n",
       "      <td>1282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21596</th>\n",
       "      <td>7502800100</td>\n",
       "      <td>20140813T000000</td>\n",
       "      <td>679950.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3600</td>\n",
       "      <td>9437</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>3600</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98059</td>\n",
       "      <td>47.4822</td>\n",
       "      <td>-122.131</td>\n",
       "      <td>3550</td>\n",
       "      <td>9421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21597</th>\n",
       "      <td>191100405</td>\n",
       "      <td>20150421T000000</td>\n",
       "      <td>1575000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3410</td>\n",
       "      <td>10125</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>3410</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>98040</td>\n",
       "      <td>47.5653</td>\n",
       "      <td>-122.223</td>\n",
       "      <td>2290</td>\n",
       "      <td>10125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21598</th>\n",
       "      <td>8956200760</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>541800.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3118</td>\n",
       "      <td>7866</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>3118</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98001</td>\n",
       "      <td>47.2931</td>\n",
       "      <td>-122.264</td>\n",
       "      <td>2673</td>\n",
       "      <td>6500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21599</th>\n",
       "      <td>7202300110</td>\n",
       "      <td>20140915T000000</td>\n",
       "      <td>810000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3990</td>\n",
       "      <td>7838</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>3990</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>98053</td>\n",
       "      <td>47.6857</td>\n",
       "      <td>-122.046</td>\n",
       "      <td>3370</td>\n",
       "      <td>6814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21600</th>\n",
       "      <td>249000205</td>\n",
       "      <td>20141015T000000</td>\n",
       "      <td>1537000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4470</td>\n",
       "      <td>8088</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>4470</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98004</td>\n",
       "      <td>47.6321</td>\n",
       "      <td>-122.200</td>\n",
       "      <td>2780</td>\n",
       "      <td>8964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21601</th>\n",
       "      <td>5100403806</td>\n",
       "      <td>20150407T000000</td>\n",
       "      <td>467000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1425</td>\n",
       "      <td>1179</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1425</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.6963</td>\n",
       "      <td>-122.318</td>\n",
       "      <td>1285</td>\n",
       "      <td>1253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21602</th>\n",
       "      <td>844000965</td>\n",
       "      <td>20140626T000000</td>\n",
       "      <td>224000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1500</td>\n",
       "      <td>11968</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98010</td>\n",
       "      <td>47.3095</td>\n",
       "      <td>-122.002</td>\n",
       "      <td>1320</td>\n",
       "      <td>11303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21603</th>\n",
       "      <td>7852140040</td>\n",
       "      <td>20140825T000000</td>\n",
       "      <td>507250.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2270</td>\n",
       "      <td>5536</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2270</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>98065</td>\n",
       "      <td>47.5389</td>\n",
       "      <td>-121.881</td>\n",
       "      <td>2270</td>\n",
       "      <td>5731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21604</th>\n",
       "      <td>9834201367</td>\n",
       "      <td>20150126T000000</td>\n",
       "      <td>429000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1490</td>\n",
       "      <td>1126</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1490</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5699</td>\n",
       "      <td>-122.288</td>\n",
       "      <td>1400</td>\n",
       "      <td>1230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21605</th>\n",
       "      <td>3448900210</td>\n",
       "      <td>20141014T000000</td>\n",
       "      <td>610685.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2520</td>\n",
       "      <td>6023</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2520</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98056</td>\n",
       "      <td>47.5137</td>\n",
       "      <td>-122.167</td>\n",
       "      <td>2520</td>\n",
       "      <td>6023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21606</th>\n",
       "      <td>7936000429</td>\n",
       "      <td>20150326T000000</td>\n",
       "      <td>1007500.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3510</td>\n",
       "      <td>7200</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2600</td>\n",
       "      <td>910</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5537</td>\n",
       "      <td>-122.398</td>\n",
       "      <td>2050</td>\n",
       "      <td>6200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21607</th>\n",
       "      <td>2997800021</td>\n",
       "      <td>20150219T000000</td>\n",
       "      <td>475000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1310</td>\n",
       "      <td>1294</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1180</td>\n",
       "      <td>130</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98116</td>\n",
       "      <td>47.5773</td>\n",
       "      <td>-122.409</td>\n",
       "      <td>1330</td>\n",
       "      <td>1265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21608</th>\n",
       "      <td>263000018</td>\n",
       "      <td>20140521T000000</td>\n",
       "      <td>360000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1530</td>\n",
       "      <td>1131</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1530</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6993</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1530</td>\n",
       "      <td>1509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21609</th>\n",
       "      <td>6600060120</td>\n",
       "      <td>20150223T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2310</td>\n",
       "      <td>5813</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2310</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98146</td>\n",
       "      <td>47.5107</td>\n",
       "      <td>-122.362</td>\n",
       "      <td>1830</td>\n",
       "      <td>7200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21610</th>\n",
       "      <td>1523300141</td>\n",
       "      <td>20140623T000000</td>\n",
       "      <td>402101.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1350</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5944</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21611</th>\n",
       "      <td>291310100</td>\n",
       "      <td>20150116T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1600</td>\n",
       "      <td>2388</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>98027</td>\n",
       "      <td>47.5345</td>\n",
       "      <td>-122.069</td>\n",
       "      <td>1410</td>\n",
       "      <td>1287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21612</th>\n",
       "      <td>1523300157</td>\n",
       "      <td>20141015T000000</td>\n",
       "      <td>325000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1076</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5941</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "      <td>1357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21613 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id             date      price  bedrooms  bathrooms  \\\n",
       "0      7129300520  20141013T000000   221900.0         3       1.00   \n",
       "1      6414100192  20141209T000000   538000.0         3       2.25   \n",
       "2      5631500400  20150225T000000   180000.0         2       1.00   \n",
       "3      2487200875  20141209T000000   604000.0         4       3.00   \n",
       "4      1954400510  20150218T000000   510000.0         3       2.00   \n",
       "5      7237550310  20140512T000000  1225000.0         4       4.50   \n",
       "6      1321400060  20140627T000000   257500.0         3       2.25   \n",
       "7      2008000270  20150115T000000   291850.0         3       1.50   \n",
       "8      2414600126  20150415T000000   229500.0         3       1.00   \n",
       "9      3793500160  20150312T000000   323000.0         3       2.50   \n",
       "10     1736800520  20150403T000000   662500.0         3       2.50   \n",
       "11     9212900260  20140527T000000   468000.0         2       1.00   \n",
       "12      114101516  20140528T000000   310000.0         3       1.00   \n",
       "13     6054650070  20141007T000000   400000.0         3       1.75   \n",
       "14     1175000570  20150312T000000   530000.0         5       2.00   \n",
       "15     9297300055  20150124T000000   650000.0         4       3.00   \n",
       "16     1875500060  20140731T000000   395000.0         3       2.00   \n",
       "17     6865200140  20140529T000000   485000.0         4       1.00   \n",
       "18       16000397  20141205T000000   189000.0         2       1.00   \n",
       "19     7983200060  20150424T000000   230000.0         3       1.00   \n",
       "20     6300500875  20140514T000000   385000.0         4       1.75   \n",
       "21     2524049179  20140826T000000  2000000.0         3       2.75   \n",
       "22     7137970340  20140703T000000   285000.0         5       2.50   \n",
       "23     8091400200  20140516T000000   252700.0         2       1.50   \n",
       "24     3814700200  20141120T000000   329000.0         3       2.25   \n",
       "25     1202000200  20141103T000000   233000.0         3       2.00   \n",
       "26     1794500383  20140626T000000   937000.0         3       1.75   \n",
       "27     3303700376  20141201T000000   667000.0         3       1.00   \n",
       "28     5101402488  20140624T000000   438000.0         3       1.75   \n",
       "29     1873100390  20150302T000000   719000.0         4       2.50   \n",
       "...           ...              ...        ...       ...        ...   \n",
       "21583  2025049203  20140610T000000   399950.0         2       1.00   \n",
       "21584   952006823  20141202T000000   380000.0         3       2.50   \n",
       "21585  3832050760  20140828T000000   270000.0         3       2.50   \n",
       "21586  2767604724  20141015T000000   505000.0         2       2.50   \n",
       "21587  6632300207  20150305T000000   385000.0         3       2.50   \n",
       "21588  2767600688  20141113T000000   414500.0         2       1.50   \n",
       "21589  7570050450  20140910T000000   347500.0         3       2.50   \n",
       "21590  7430200100  20140514T000000  1222500.0         4       3.50   \n",
       "21591  4140940150  20141002T000000   572000.0         4       2.75   \n",
       "21592  1931300412  20150416T000000   475000.0         3       2.25   \n",
       "21593  8672200110  20150317T000000  1088000.0         5       3.75   \n",
       "21594  5087900040  20141017T000000   350000.0         4       2.75   \n",
       "21595  1972201967  20141031T000000   520000.0         2       2.25   \n",
       "21596  7502800100  20140813T000000   679950.0         5       2.75   \n",
       "21597   191100405  20150421T000000  1575000.0         4       3.25   \n",
       "21598  8956200760  20141013T000000   541800.0         4       2.50   \n",
       "21599  7202300110  20140915T000000   810000.0         4       3.00   \n",
       "21600   249000205  20141015T000000  1537000.0         5       3.75   \n",
       "21601  5100403806  20150407T000000   467000.0         3       2.50   \n",
       "21602   844000965  20140626T000000   224000.0         3       1.75   \n",
       "21603  7852140040  20140825T000000   507250.0         3       2.50   \n",
       "21604  9834201367  20150126T000000   429000.0         3       2.00   \n",
       "21605  3448900210  20141014T000000   610685.0         4       2.50   \n",
       "21606  7936000429  20150326T000000  1007500.0         4       3.50   \n",
       "21607  2997800021  20150219T000000   475000.0         3       2.50   \n",
       "21608   263000018  20140521T000000   360000.0         3       2.50   \n",
       "21609  6600060120  20150223T000000   400000.0         4       2.50   \n",
       "21610  1523300141  20140623T000000   402101.0         2       0.75   \n",
       "21611   291310100  20150116T000000   400000.0         3       2.50   \n",
       "21612  1523300157  20141015T000000   325000.0         2       0.75   \n",
       "\n",
       "       sqft_living  sqft_lot  floors  waterfront  view     ...      grade  \\\n",
       "0             1180      5650     1.0           0     0     ...          7   \n",
       "1             2570      7242     2.0           0     0     ...          7   \n",
       "2              770     10000     1.0           0     0     ...          6   \n",
       "3             1960      5000     1.0           0     0     ...          7   \n",
       "4             1680      8080     1.0           0     0     ...          8   \n",
       "5             5420    101930     1.0           0     0     ...         11   \n",
       "6             1715      6819     2.0           0     0     ...          7   \n",
       "7             1060      9711     1.0           0     0     ...          7   \n",
       "8             1780      7470     1.0           0     0     ...          7   \n",
       "9             1890      6560     2.0           0     0     ...          7   \n",
       "10            3560      9796     1.0           0     0     ...          8   \n",
       "11            1160      6000     1.0           0     0     ...          7   \n",
       "12            1430     19901     1.5           0     0     ...          7   \n",
       "13            1370      9680     1.0           0     0     ...          7   \n",
       "14            1810      4850     1.5           0     0     ...          7   \n",
       "15            2950      5000     2.0           0     3     ...          9   \n",
       "16            1890     14040     2.0           0     0     ...          7   \n",
       "17            1600      4300     1.5           0     0     ...          7   \n",
       "18            1200      9850     1.0           0     0     ...          7   \n",
       "19            1250      9774     1.0           0     0     ...          7   \n",
       "20            1620      4980     1.0           0     0     ...          7   \n",
       "21            3050     44867     1.0           0     4     ...          9   \n",
       "22            2270      6300     2.0           0     0     ...          8   \n",
       "23            1070      9643     1.0           0     0     ...          7   \n",
       "24            2450      6500     2.0           0     0     ...          8   \n",
       "25            1710      4697     1.5           0     0     ...          6   \n",
       "26            2450      2691     2.0           0     0     ...          8   \n",
       "27            1400      1581     1.5           0     0     ...          8   \n",
       "28            1520      6380     1.0           0     0     ...          7   \n",
       "29            2570      7173     2.0           0     0     ...          8   \n",
       "...            ...       ...     ...         ...   ...     ...        ...   \n",
       "21583          710      1157     2.0           0     0     ...          7   \n",
       "21584         1260       900     2.0           0     0     ...          7   \n",
       "21585         1870      5000     2.0           0     0     ...          7   \n",
       "21586         1430      1201     3.0           0     0     ...          8   \n",
       "21587         1520      1488     3.0           0     0     ...          8   \n",
       "21588         1210      1278     2.0           0     0     ...          8   \n",
       "21589         2540      4760     2.0           0     0     ...          8   \n",
       "21590         4910      9444     1.5           0     0     ...         11   \n",
       "21591         2770      3852     2.0           0     0     ...          8   \n",
       "21592         1190      1200     3.0           0     0     ...          8   \n",
       "21593         4170      8142     2.0           0     2     ...         10   \n",
       "21594         2500      5995     2.0           0     0     ...          8   \n",
       "21595         1530       981     3.0           0     0     ...          8   \n",
       "21596         3600      9437     2.0           0     0     ...          9   \n",
       "21597         3410     10125     2.0           0     0     ...         10   \n",
       "21598         3118      7866     2.0           0     2     ...          9   \n",
       "21599         3990      7838     2.0           0     0     ...          9   \n",
       "21600         4470      8088     2.0           0     0     ...         11   \n",
       "21601         1425      1179     3.0           0     0     ...          8   \n",
       "21602         1500     11968     1.0           0     0     ...          6   \n",
       "21603         2270      5536     2.0           0     0     ...          8   \n",
       "21604         1490      1126     3.0           0     0     ...          8   \n",
       "21605         2520      6023     2.0           0     0     ...          9   \n",
       "21606         3510      7200     2.0           0     0     ...          9   \n",
       "21607         1310      1294     2.0           0     0     ...          8   \n",
       "21608         1530      1131     3.0           0     0     ...          8   \n",
       "21609         2310      5813     2.0           0     0     ...          8   \n",
       "21610         1020      1350     2.0           0     0     ...          7   \n",
       "21611         1600      2388     2.0           0     0     ...          8   \n",
       "21612         1020      1076     2.0           0     0     ...          7   \n",
       "\n",
       "       sqft_above  sqft_basement  yr_built  yr_renovated  zipcode      lat  \\\n",
       "0            1180              0      1955             0    98178  47.5112   \n",
       "1            2170            400      1951          1991    98125  47.7210   \n",
       "2             770              0      1933             0    98028  47.7379   \n",
       "3            1050            910      1965             0    98136  47.5208   \n",
       "4            1680              0      1987             0    98074  47.6168   \n",
       "5            3890           1530      2001             0    98053  47.6561   \n",
       "6            1715              0      1995             0    98003  47.3097   \n",
       "7            1060              0      1963             0    98198  47.4095   \n",
       "8            1050            730      1960             0    98146  47.5123   \n",
       "9            1890              0      2003             0    98038  47.3684   \n",
       "10           1860           1700      1965             0    98007  47.6007   \n",
       "11            860            300      1942             0    98115  47.6900   \n",
       "12           1430              0      1927             0    98028  47.7558   \n",
       "13           1370              0      1977             0    98074  47.6127   \n",
       "14           1810              0      1900             0    98107  47.6700   \n",
       "15           1980            970      1979             0    98126  47.5714   \n",
       "16           1890              0      1994             0    98019  47.7277   \n",
       "17           1600              0      1916             0    98103  47.6648   \n",
       "18           1200              0      1921             0    98002  47.3089   \n",
       "19           1250              0      1969             0    98003  47.3343   \n",
       "20            860            760      1947             0    98133  47.7025   \n",
       "21           2330            720      1968             0    98040  47.5316   \n",
       "22           2270              0      1995             0    98092  47.3266   \n",
       "23           1070              0      1985             0    98030  47.3533   \n",
       "24           2450              0      1985             0    98030  47.3739   \n",
       "25           1710              0      1941             0    98002  47.3048   \n",
       "26           1750            700      1915             0    98119  47.6386   \n",
       "27           1400              0      1909             0    98112  47.6221   \n",
       "28            790            730      1948             0    98115  47.6950   \n",
       "29           2570              0      2005             0    98052  47.7073   \n",
       "...           ...            ...       ...           ...      ...      ...   \n",
       "21583         710              0      1943             0    98102  47.6413   \n",
       "21584         940            320      2007             0    98116  47.5621   \n",
       "21585        1870              0      2009             0    98042  47.3339   \n",
       "21586        1430              0      2009             0    98107  47.6707   \n",
       "21587        1520              0      2006             0    98125  47.7337   \n",
       "21588        1020            190      2007             0    98117  47.6756   \n",
       "21589        2540              0      2010             0    98038  47.3452   \n",
       "21590        3110           1800      2007             0    98074  47.6502   \n",
       "21591        2770              0      2014             0    98178  47.5001   \n",
       "21592        1190              0      2008             0    98103  47.6542   \n",
       "21593        4170              0      2006             0    98056  47.5354   \n",
       "21594        2500              0      2008             0    98042  47.3749   \n",
       "21595        1480             50      2006             0    98103  47.6533   \n",
       "21596        3600              0      2014             0    98059  47.4822   \n",
       "21597        3410              0      2007             0    98040  47.5653   \n",
       "21598        3118              0      2014             0    98001  47.2931   \n",
       "21599        3990              0      2003             0    98053  47.6857   \n",
       "21600        4470              0      2008             0    98004  47.6321   \n",
       "21601        1425              0      2008             0    98125  47.6963   \n",
       "21602        1500              0      2014             0    98010  47.3095   \n",
       "21603        2270              0      2003             0    98065  47.5389   \n",
       "21604        1490              0      2014             0    98144  47.5699   \n",
       "21605        2520              0      2014             0    98056  47.5137   \n",
       "21606        2600            910      2009             0    98136  47.5537   \n",
       "21607        1180            130      2008             0    98116  47.5773   \n",
       "21608        1530              0      2009             0    98103  47.6993   \n",
       "21609        2310              0      2014             0    98146  47.5107   \n",
       "21610        1020              0      2009             0    98144  47.5944   \n",
       "21611        1600              0      2004             0    98027  47.5345   \n",
       "21612        1020              0      2008             0    98144  47.5941   \n",
       "\n",
       "          long  sqft_living15  sqft_lot15  \n",
       "0     -122.257           1340        5650  \n",
       "1     -122.319           1690        7639  \n",
       "2     -122.233           2720        8062  \n",
       "3     -122.393           1360        5000  \n",
       "4     -122.045           1800        7503  \n",
       "5     -122.005           4760      101930  \n",
       "6     -122.327           2238        6819  \n",
       "7     -122.315           1650        9711  \n",
       "8     -122.337           1780        8113  \n",
       "9     -122.031           2390        7570  \n",
       "10    -122.145           2210        8925  \n",
       "11    -122.292           1330        6000  \n",
       "12    -122.229           1780       12697  \n",
       "13    -122.045           1370       10208  \n",
       "14    -122.394           1360        4850  \n",
       "15    -122.375           2140        4000  \n",
       "16    -121.962           1890       14018  \n",
       "17    -122.343           1610        4300  \n",
       "18    -122.210           1060        5095  \n",
       "19    -122.306           1280        8850  \n",
       "20    -122.341           1400        4980  \n",
       "21    -122.233           4110       20336  \n",
       "22    -122.169           2240        7005  \n",
       "23    -122.166           1220        8386  \n",
       "24    -122.172           2200        6865  \n",
       "25    -122.218           1030        4705  \n",
       "26    -122.360           1760        3573  \n",
       "27    -122.314           1860        3861  \n",
       "28    -122.304           1520        6235  \n",
       "29    -122.110           2630        6026  \n",
       "...        ...            ...         ...  \n",
       "21583 -122.329           1370        1173  \n",
       "21584 -122.384           1310        1415  \n",
       "21585 -122.055           2170        5399  \n",
       "21586 -122.381           1430        1249  \n",
       "21587 -122.309           1520        1497  \n",
       "21588 -122.375           1210        1118  \n",
       "21589 -122.022           2540        4571  \n",
       "21590 -122.066           4560       11063  \n",
       "21591 -122.232           1810        5641  \n",
       "21592 -122.346           1180        1224  \n",
       "21593 -122.181           3030        7980  \n",
       "21594 -122.107           2530        5988  \n",
       "21595 -122.346           1530        1282  \n",
       "21596 -122.131           3550        9421  \n",
       "21597 -122.223           2290       10125  \n",
       "21598 -122.264           2673        6500  \n",
       "21599 -122.046           3370        6814  \n",
       "21600 -122.200           2780        8964  \n",
       "21601 -122.318           1285        1253  \n",
       "21602 -122.002           1320       11303  \n",
       "21603 -121.881           2270        5731  \n",
       "21604 -122.288           1400        1230  \n",
       "21605 -122.167           2520        6023  \n",
       "21606 -122.398           2050        6200  \n",
       "21607 -122.409           1330        1265  \n",
       "21608 -122.346           1530        1509  \n",
       "21609 -122.362           1830        7200  \n",
       "21610 -122.299           1020        2007  \n",
       "21611 -122.069           1410        1287  \n",
       "21612 -122.299           1020        1357  \n",
       "\n",
       "[21613 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('kc_house_data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1955, 1951, 1933, 1965, 1987, 2001, 1995, 1963, 1960, 2003, 1942,\n",
       "       1927, 1977, 1900, 1979, 1994, 1916, 1921, 1969, 1947, 1968, 1985,\n",
       "       1941, 1915, 1909, 1948, 2005, 1929, 1981, 1930, 1904, 1996, 2000,\n",
       "       1984, 2014, 1922, 1959, 1966, 1953, 1950, 2008, 1991, 1954, 1973,\n",
       "       1925, 1989, 1972, 1986, 1956, 2002, 1992, 1964, 1952, 1961, 2006,\n",
       "       1988, 1962, 1939, 1946, 1967, 1975, 1980, 1910, 1983, 1978, 1905,\n",
       "       1971, 2010, 1945, 1924, 1990, 1914, 1926, 2004, 1923, 2007, 1976,\n",
       "       1949, 1999, 1901, 1993, 1920, 1997, 1943, 1957, 1940, 1918, 1928,\n",
       "       1974, 1911, 1936, 1937, 1982, 1908, 1931, 1998, 1913, 2013, 1907,\n",
       "       1958, 2012, 1912, 2011, 1917, 1932, 1944, 1902, 2009, 1903, 1970,\n",
       "       2015, 1934, 1938, 1919, 1906, 1935], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"yr_built\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_selection = [\"bedrooms\",\"bathrooms\",\"sqft_living\",\"sqft_lot\",\n",
    "                    \"floors\",\"condition\",\"grade\",\"sqft_above\",\n",
    "                    \"sqft_basement\",\"sqft_living15\",\"sqft_lot15\",\n",
    "                    \"lat\", \"long\",\"yr_built\",\"yr_renovated\",\"waterfront\"]\n",
    "\n",
    "selected_feature = np.array(data[column_selection])\n",
    "price = np.array(data[\"price\"])\n",
    "selected_feature_train = selected_feature[:20000]\n",
    "price_train = price[:20000]\n",
    "\n",
    "selected_feature_test = selected_feature[20000:]\n",
    "price_test = price[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y,y_pred):\n",
    "    return np.mean(np.abs(y-y_pred)/y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_len = len(column_selection)\n",
    "model.add(keras.layers.Dense(50, input_dim=input_len, activation='relu'))\n",
    "model.add(keras.layers.Dense(50, activation='relu'))\n",
    "model.add(keras.layers.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "20000/20000 [==============================] - 1s 48us/step - loss: 362423227672.1664\n",
      "Epoch 2/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 241266003673.0880\n",
      "Epoch 3/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 104657655149.3632\n",
      "Epoch 4/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 76096611719.5776\n",
      "Epoch 5/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 74086634251.8784\n",
      "Epoch 6/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 72355529228.2880\n",
      "Epoch 7/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 70839192594.0224\n",
      "Epoch 8/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 69601997766.6560\n",
      "Epoch 9/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 68416932983.6032\n",
      "Epoch 10/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 67236469722.3168\n",
      "Epoch 11/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 66372671596.1344\n",
      "Epoch 12/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 65672606646.2720\n",
      "Epoch 13/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 65128629390.5408\n",
      "Epoch 14/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 64582580106.0352\n",
      "Epoch 15/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 64184697303.8592\n",
      "Epoch 16/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 63889603126.8864\n",
      "Epoch 17/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 63805269973.4016\n",
      "Epoch 18/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 63460876900.7616\n",
      "Epoch 19/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 63324490366.9760\n",
      "Epoch 20/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 63048134321.7664\n",
      "Epoch 21/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 62751331519.6928\n",
      "Epoch 22/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 62455936004.9152\n",
      "Epoch 23/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 62417944589.1072\n",
      "Epoch 24/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 62189422287.2576\n",
      "Epoch 25/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 61865934422.0160\n",
      "Epoch 26/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 61621737881.6000\n",
      "Epoch 27/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 61332560871.4240\n",
      "Epoch 28/50\n",
      "20000/20000 [==============================] - 0s 14us/step - loss: 60933667232.1536\n",
      "Epoch 29/50\n",
      "20000/20000 [==============================] - 0s 13us/step - loss: 60915383461.4784\n",
      "Epoch 30/50\n",
      "20000/20000 [==============================] - 0s 13us/step - loss: 60524351337.2672\n",
      "Epoch 31/50\n",
      "20000/20000 [==============================] - 0s 14us/step - loss: 60173872372.1216\n",
      "Epoch 32/50\n",
      "20000/20000 [==============================] - 0s 13us/step - loss: 59914683246.1824\n",
      "Epoch 33/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 59413763280.0768\n",
      "Epoch 34/50\n",
      "20000/20000 [==============================] - 0s 13us/step - loss: 59187383107.5840\n",
      "Epoch 35/50\n",
      "20000/20000 [==============================] - 0s 14us/step - loss: 58804822979.3792\n",
      "Epoch 36/50\n",
      "20000/20000 [==============================] - 0s 14us/step - loss: 58427143749.6320\n",
      "Epoch 37/50\n",
      "20000/20000 [==============================] - 0s 13us/step - loss: 58193626582.2208\n",
      "Epoch 38/50\n",
      "20000/20000 [==============================] - 0s 15us/step - loss: 57860970958.0288\n",
      "Epoch 39/50\n",
      "20000/20000 [==============================] - 0s 13us/step - loss: 57484226763.1616\n",
      "Epoch 40/50\n",
      "20000/20000 [==============================] - 0s 13us/step - loss: 57240758937.1904\n",
      "Epoch 41/50\n",
      "20000/20000 [==============================] - 0s 12us/step - loss: 57191458681.6512\n",
      "Epoch 42/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 56880555216.0768\n",
      "Epoch 43/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 56534853576.2944\n",
      "Epoch 44/50\n",
      "20000/20000 [==============================] - 0s 11us/step - loss: 56118989907.5584\n",
      "Epoch 45/50\n",
      "20000/20000 [==============================] - 0s 13us/step - loss: 56056098573.5168\n",
      "Epoch 46/50\n",
      "20000/20000 [==============================] - 0s 14us/step - loss: 56078933458.9440\n",
      "Epoch 47/50\n",
      "20000/20000 [==============================] - 0s 13us/step - loss: 56090581958.6560\n",
      "Epoch 48/50\n",
      "20000/20000 [==============================] - 0s 13us/step - loss: 55637524047.4624\n",
      "Epoch 49/50\n",
      "20000/20000 [==============================] - 0s 11us/step - loss: 55694483128.3200\n",
      "Epoch 50/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 55447747251.4048\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(selected_feature_train, price_train,\n",
    "                        epochs=50, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2354.81517370684"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(selected_feature_test)\n",
    "score(preds, price_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Like SKLearn, it's easy to train and evaluate simple models.\n",
    "#### ... but we should try to do better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Deep Learning -- What you need to know\n",
    "### Train, Validation, Test:\n",
    "   * Optimize parameters with Train (weights, biases)\n",
    "   * Optimize hyperparameters with Validation (layer width & depth, activation functions, etc.)\n",
    "   * Optimize NOTHING with Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split out a validation set for hyperparameter optimization\n",
    "\n",
    "selected_feature_train = selected_feature[:18000]\n",
    "price_train = price[:18000]\n",
    "selected_feature_val = selected_feature[18000:20000]\n",
    "price_val = price[18000:20000]\n",
    "selected_feature_test = selected_feature[20000:]\n",
    "price_test = price[20000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a hyperparameter optimization:\n",
    "\n",
    "### Try three activation functions to use for dense layers in the neural network above. Save the model that achieves the best validation loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint: [activation functions](http://letmegooglethat.com/?q=keras+activation+functions)\n",
    "\n",
    "#### Hint: `model.fit` has argument \"`validation_data`\" which takes a tuple of features and targets\n",
    "\n",
    "#### Hint: Use `model.save(\"filename.h5\")` to save a model locally. If you want to use it later, just call `keras.models.load_model(\"filename.h5\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For easy looping, define neural network model as a function\n",
    "def nn_model(optimizer='adam',\n",
    "             activation='relu',\n",
    "             layers=[20,20],\n",
    "             loss='mean_squared_error'):\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(50, input_dim=input_len, activation=activ))\n",
    "    model.add(keras.layers.Dense(50, activation=activ))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "\n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "18000/18000 [==============================] - 1s 31us/step - loss: 532974.0115 - val_loss: 557914.0565\n",
      "Epoch 2/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532966.4315 - val_loss: 557907.7440\n",
      "Epoch 3/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532960.4552 - val_loss: 557902.0385\n",
      "Epoch 4/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532954.8337 - val_loss: 557896.6045\n",
      "Epoch 5/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532949.3721 - val_loss: 557891.0965\n",
      "Epoch 6/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 532943.7817 - val_loss: 557885.2920\n",
      "Epoch 7/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532938.0435 - val_loss: 557879.6830\n",
      "Epoch 8/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532932.4725 - val_loss: 557874.1570\n",
      "Epoch 9/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532926.9642 - val_loss: 557868.6770\n",
      "Epoch 10/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532921.4915 - val_loss: 557863.1610\n",
      "Epoch 11/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532916.0389 - val_loss: 557857.7420\n",
      "Epoch 12/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532910.6117 - val_loss: 557852.3100\n",
      "Epoch 13/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532905.1918 - val_loss: 557846.8370\n",
      "Epoch 14/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532899.7812 - val_loss: 557841.6045\n",
      "Epoch 15/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532894.3781 - val_loss: 557836.1210\n",
      "Epoch 16/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532888.9848 - val_loss: 557830.7190\n",
      "Epoch 17/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532883.5964 - val_loss: 557825.3100\n",
      "Epoch 18/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532878.2143 - val_loss: 557819.8590\n",
      "Epoch 19/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532872.8313 - val_loss: 557814.6165\n",
      "Epoch 20/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532867.4610 - val_loss: 557809.1750\n",
      "Epoch 21/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 532862.0847 - val_loss: 557803.7820\n",
      "Epoch 22/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 532856.7184 - val_loss: 557798.3910\n",
      "Epoch 23/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532851.3425 - val_loss: 557793.0965\n",
      "Epoch 24/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532845.9747 - val_loss: 557787.7190\n",
      "Epoch 25/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532840.6135 - val_loss: 557782.3140\n",
      "Epoch 26/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532835.2472 - val_loss: 557776.8650\n",
      "Epoch 27/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532829.8720 - val_loss: 557771.6770\n",
      "Epoch 28/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 532824.5156 - val_loss: 557766.2240\n",
      "Epoch 29/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532819.1571 - val_loss: 557760.8125\n",
      "Epoch 30/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532813.7879 - val_loss: 557755.6125\n",
      "Epoch 31/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532808.4271 - val_loss: 557750.1610\n",
      "Epoch 32/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532803.0678 - val_loss: 557744.7680\n",
      "Epoch 33/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 532797.7111 - val_loss: 557739.3910\n",
      "Epoch 34/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532792.3457 - val_loss: 557734.0965\n",
      "Epoch 35/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532786.9829 - val_loss: 557728.7420\n",
      "Epoch 36/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532781.6269 - val_loss: 557723.3160\n",
      "Epoch 37/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532776.2653 - val_loss: 557717.8690\n",
      "Epoch 38/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532770.9041 - val_loss: 557712.6810\n",
      "Epoch 39/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532765.5489 - val_loss: 557707.2920\n",
      "Epoch 40/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 532760.1891 - val_loss: 557701.8570\n",
      "Epoch 41/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532754.8321 - val_loss: 557696.6165\n",
      "Epoch 42/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532749.4747 - val_loss: 557691.1790\n",
      "Epoch 43/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 532744.1129 - val_loss: 557685.7840\n",
      "Epoch 44/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 532738.7598 - val_loss: 557680.6045\n",
      "Epoch 45/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532733.3988 - val_loss: 557675.1570\n",
      "Epoch 46/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532728.0400 - val_loss: 557669.7480\n",
      "Epoch 47/50\n",
      "18000/18000 [==============================] - ETA: 0s - loss: 532702.63 - 0s 10us/step - loss: 532722.6843 - val_loss: 557664.3870\n",
      "Epoch 48/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532717.3249 - val_loss: 557659.0945\n",
      "Epoch 49/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532711.9656 - val_loss: 557653.7170\n",
      "Epoch 50/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532706.6133 - val_loss: 557648.3140\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "18000/18000 [==============================] - 0s 24us/step - loss: 532970.1318 - val_loss: 557909.2725\n",
      "Epoch 2/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532960.9386 - val_loss: 557901.5785\n",
      "Epoch 3/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532953.3493 - val_loss: 557894.0385\n",
      "Epoch 4/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532945.9204 - val_loss: 557886.6870\n",
      "Epoch 5/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532938.5780 - val_loss: 557879.3140\n",
      "Epoch 6/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532931.2692 - val_loss: 557872.0885\n",
      "Epoch 7/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532923.9883 - val_loss: 557864.7460\n",
      "Epoch 8/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532916.7349 - val_loss: 557857.6045\n",
      "Epoch 9/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532909.4831 - val_loss: 557850.2620\n",
      "Epoch 10/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532902.2473 - val_loss: 557843.0885\n",
      "Epoch 11/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532895.0113 - val_loss: 557835.7840\n",
      "Epoch 12/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 532887.7956 - val_loss: 557828.6510\n",
      "Epoch 13/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532880.5834 - val_loss: 557821.3265\n",
      "Epoch 14/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532873.3627 - val_loss: 557814.1610\n",
      "Epoch 15/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532866.1591 - val_loss: 557806.8590\n",
      "Epoch 16/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532858.9509 - val_loss: 557799.7420\n",
      "Epoch 17/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532851.7448 - val_loss: 557792.6145\n",
      "Epoch 18/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 532844.5419 - val_loss: 557785.3140\n",
      "Epoch 19/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 532837.3457 - val_loss: 557778.1610\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 0s 10us/step - loss: 532830.1402 - val_loss: 557770.8590\n",
      "Epoch 21/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532822.9454 - val_loss: 557763.7420\n",
      "Epoch 22/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532815.7507 - val_loss: 557756.6145\n",
      "Epoch 23/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532808.5511 - val_loss: 557749.3160\n",
      "Epoch 24/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532801.3553 - val_loss: 557742.1610\n",
      "Epoch 25/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532794.1591 - val_loss: 557734.8650\n",
      "Epoch 26/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532786.9602 - val_loss: 557727.7420\n",
      "Epoch 27/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 532779.7653 - val_loss: 557720.6205\n",
      "Epoch 28/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532772.5806 - val_loss: 557713.3670\n",
      "Epoch 29/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532765.3860 - val_loss: 557706.1750\n",
      "Epoch 30/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532758.1895 - val_loss: 557698.8750\n",
      "Epoch 31/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532750.9944 - val_loss: 557691.7640\n",
      "Epoch 32/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532743.8066 - val_loss: 557684.6770\n",
      "Epoch 33/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532736.6143 - val_loss: 557677.3870\n",
      "Epoch 34/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532729.4202 - val_loss: 557670.2240\n",
      "Epoch 35/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532722.2279 - val_loss: 557663.0885\n",
      "Epoch 36/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532715.0391 - val_loss: 557655.7840\n",
      "Epoch 37/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532707.8489 - val_loss: 557648.6830\n",
      "Epoch 38/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532700.6618 - val_loss: 557641.4515\n",
      "Epoch 39/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532693.4645 - val_loss: 557634.2620\n",
      "Epoch 40/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 532686.2773 - val_loss: 557627.0965\n",
      "Epoch 41/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532679.0872 - val_loss: 557619.8290\n",
      "Epoch 42/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532671.8926 - val_loss: 557612.7170\n",
      "Epoch 43/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532664.7100 - val_loss: 557605.6125\n",
      "Epoch 44/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532657.5118 - val_loss: 557598.3140\n",
      "Epoch 45/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 532650.3236 - val_loss: 557591.1570\n",
      "Epoch 46/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532643.1303 - val_loss: 557583.8590\n",
      "Epoch 47/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532635.9405 - val_loss: 557576.7420\n",
      "Epoch 48/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532628.7522 - val_loss: 557569.6165\n",
      "Epoch 49/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532621.5599 - val_loss: 557562.3200\n",
      "Epoch 50/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 532614.3712 - val_loss: 557555.1610\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "18000/18000 [==============================] - 0s 26us/step - loss: 471629.3549 - val_loss: 393616.4920\n",
      "Epoch 2/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 284965.8841 - val_loss: 210190.9591\n",
      "Epoch 3/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 174749.8863 - val_loss: 176087.3575\n",
      "Epoch 4/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 165280.5353 - val_loss: 172611.6448\n",
      "Epoch 5/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 162512.0588 - val_loss: 170917.4149\n",
      "Epoch 6/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 160908.6006 - val_loss: 169849.0967\n",
      "Epoch 7/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 159791.3595 - val_loss: 169389.2419\n",
      "Epoch 8/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 158895.5095 - val_loss: 168744.8751\n",
      "Epoch 9/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 158404.5609 - val_loss: 168205.0501\n",
      "Epoch 10/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 157909.7211 - val_loss: 169048.3570\n",
      "Epoch 11/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 157451.4323 - val_loss: 167702.8733\n",
      "Epoch 12/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 157242.4132 - val_loss: 167947.4253\n",
      "Epoch 13/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 156943.7748 - val_loss: 167810.8915\n",
      "Epoch 14/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 156826.7013 - val_loss: 167607.8302\n",
      "Epoch 15/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 156344.1622 - val_loss: 167454.1047\n",
      "Epoch 16/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 156121.3991 - val_loss: 167681.9083\n",
      "Epoch 17/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 156331.3789 - val_loss: 167325.9761\n",
      "Epoch 18/50\n",
      "18000/18000 [==============================] - ETA: 0s - loss: 157011.79 - 0s 10us/step - loss: 155831.9315 - val_loss: 167647.3609\n",
      "Epoch 19/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 155701.4959 - val_loss: 167168.8590\n",
      "Epoch 20/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 155523.9505 - val_loss: 167251.6181\n",
      "Epoch 21/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 155494.3585 - val_loss: 167212.3311\n",
      "Epoch 22/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 155445.9638 - val_loss: 166954.7932\n",
      "Epoch 23/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 155014.2676 - val_loss: 166947.6963\n",
      "Epoch 24/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 155070.8165 - val_loss: 168088.7749\n",
      "Epoch 25/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 155226.7230 - val_loss: 167022.0431\n",
      "Epoch 26/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 154751.3493 - val_loss: 166873.7321\n",
      "Epoch 27/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 154751.2427 - val_loss: 166480.2068\n",
      "Epoch 28/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 154470.2612 - val_loss: 167122.9260\n",
      "Epoch 29/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 154169.6711 - val_loss: 166378.0950\n",
      "Epoch 30/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 154632.0026 - val_loss: 166717.3799\n",
      "Epoch 31/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 154757.9446 - val_loss: 166875.5820\n",
      "Epoch 32/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 153912.6585 - val_loss: 166335.4771\n",
      "Epoch 33/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 154042.7177 - val_loss: 165991.6961\n",
      "Epoch 34/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 153673.7183 - val_loss: 165904.2189\n",
      "Epoch 35/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 153767.3106 - val_loss: 165795.5203\n",
      "Epoch 36/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 153723.8461 - val_loss: 165781.6608\n",
      "Epoch 37/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 153370.7638 - val_loss: 165706.2790\n",
      "Epoch 38/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 153425.5678 - val_loss: 165681.5013\n",
      "Epoch 39/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 153456.0816 - val_loss: 166970.5782\n",
      "Epoch 40/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 0s 10us/step - loss: 153565.2268 - val_loss: 166097.1311\n",
      "Epoch 41/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 153216.4195 - val_loss: 165510.6073\n",
      "Epoch 42/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 153325.5166 - val_loss: 166323.6279\n",
      "Epoch 43/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 153462.2408 - val_loss: 165405.9931\n",
      "Epoch 44/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 153027.9525 - val_loss: 165772.9790\n",
      "Epoch 45/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 152805.2123 - val_loss: 165447.2462\n",
      "Epoch 46/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 152676.3412 - val_loss: 165712.0539\n",
      "Epoch 47/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 152723.5669 - val_loss: 165205.6867\n",
      "Epoch 48/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 152770.5024 - val_loss: 165876.8422\n",
      "Epoch 49/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 152555.3561 - val_loss: 165403.5799\n",
      "Epoch 50/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 152461.7974 - val_loss: 165037.4793\n",
      "BEST ACTIVATION FUNCTION relu WITH SCORE 0.6260248781288189\n"
     ]
    }
   ],
   "source": [
    "best_score = 1000.0 # bad\n",
    "\n",
    "# loop over chosen activation functions, train, evaluate on validation\n",
    "for activ in ['sigmoid', 'tanh', 'relu']:\n",
    "    model = nn_model(activation=activ)\n",
    "\n",
    "    history = model.fit(selected_feature_train, price_train,\n",
    "                epochs=50, batch_size=128,\n",
    "                validation_data=(selected_feature_val, price_val))\n",
    "    model_score = score(model.predict(selected_feature_val), price_val)\n",
    "\n",
    "    if model_score < best_score:\n",
    "        best_score = model_score\n",
    "        best_activ = activ\n",
    "        best_model = model\n",
    "        best_train = history\n",
    "\n",
    "print(f\"BEST ACTIVATION FUNCTION {best_activ} WITH SCORE {best_score}\")\n",
    "best_model.save(\"awesome_model.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize your training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot loss during training\n",
    "def plot_loss(hist):\n",
    "    %matplotlib inline\n",
    "    plt.title('Training Curve')\n",
    "    plt.plot(hist.history['loss'], label='train')\n",
    "    plt.plot(hist.history['val_loss'], label='validation')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Mean squared error\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_loss(best_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the future, try better validation schemes like [k-fold cross validation](https://chrisalbon.com/deep_learning/keras/k-fold_cross-validating_neural_networks/), though 80/20 or 90/10 train/val like this works in a pinch\n",
    "\n",
    "### Standardize your features:\n",
    "* Typically assumes normally distributed feature, shifting mean to 0 and standard deviation to 1\n",
    "* In theory does not matter for neural networks\n",
    "* In practice tends to matter for neural networks\n",
    "* Scale if using:\n",
    "    - Logistic regression\n",
    "    - Support vector machines\n",
    "    - Perceptrons\n",
    "    - Neural networks\n",
    "    - Principle component analysis\n",
    "* Don't bother if using:\n",
    "    - \"Forest\" methods\n",
    "    - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.5395092608135503e-17\n",
      "1.0\n",
      "0.016119707489855948\n",
      "0.9970087526331802\n",
      "0.09330835574898905\n",
      "0.9414925643047213\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Instantiate StandardScaler\n",
    "in_scaler = StandardScaler()\n",
    "\n",
    "# Fit scaler to the training set and perform the transformation\n",
    "selected_feature_train = in_scaler.fit_transform(selected_feature_train)\n",
    "\n",
    "# Use the fitted scaler to transform validation and test features\n",
    "selected_feature_val = in_scaler.transform(selected_feature_val)\n",
    "selected_feature_test = in_scaler.transform(selected_feature_test)\n",
    "\n",
    "# Check appropriate scaling\n",
    "print(np.mean(selected_feature_train[:,0]))\n",
    "print(np.std(selected_feature_train[:,0]))\n",
    "\n",
    "print(np.mean(selected_feature_val[:,0]))\n",
    "print(np.std(selected_feature_val[:,0]))\n",
    "\n",
    "print(np.mean(selected_feature_test[:,0]))\n",
    "print(np.std(selected_feature_test[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/200\n",
      "18000/18000 [==============================] - 1s 30us/step - loss: 416643360664.2347 - val_loss: 456226459090.9440\n",
      "Epoch 2/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416175119983.5022 - val_loss: 455096928829.4401\n",
      "Epoch 3/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 414325133541.3760 - val_loss: 451825760731.1359\n",
      "Epoch 4/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 410124513735.1111 - val_loss: 445473112981.5040\n",
      "Epoch 5/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 402882324099.5271 - val_loss: 435331354329.0880\n",
      "Epoch 6/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 391998989372.9849 - val_loss: 420808691810.3040\n",
      "Epoch 7/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 377252354399.3458 - val_loss: 401978543308.8000\n",
      "Epoch 8/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 358771684627.7974 - val_loss: 379162710769.6640\n",
      "Epoch 9/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 336863679101.1555 - val_loss: 352851277381.6320\n",
      "Epoch 10/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 312167480018.7164 - val_loss: 323778086699.0080\n",
      "Epoch 11/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 285482632231.1395 - val_loss: 292908206391.2960\n",
      "Epoch 12/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 257900540650.3822 - val_loss: 261877699706.8800\n",
      "Epoch 13/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 230439534956.0889 - val_loss: 231827165806.5920\n",
      "Epoch 14/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 204230289755.7049 - val_loss: 203510794944.5120\n",
      "Epoch 15/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 180171257282.5600 - val_loss: 178228993720.3200\n",
      "Epoch 16/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 158859801220.4373 - val_loss: 156422493044.7360\n",
      "Epoch 17/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 140660142425.8845 - val_loss: 138249369092.0960\n",
      "Epoch 18/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 125589164647.3102 - val_loss: 123814903939.0720\n",
      "Epoch 19/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 113501671595.1218 - val_loss: 112468381204.4800\n",
      "Epoch 20/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 104093540628.7076 - val_loss: 103961095700.4800\n",
      "Epoch 21/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 96870897294.4498 - val_loss: 97507325378.5600\n",
      "Epoch 22/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 91388252200.0498 - val_loss: 92750705197.0560\n",
      "Epoch 23/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 87164592739.6693 - val_loss: 89085560487.9360\n",
      "Epoch 24/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 83875793644.2027 - val_loss: 86196380237.8240\n",
      "Epoch 25/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 81264161499.8187 - val_loss: 83851038294.0160\n",
      "Epoch 26/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 79161698010.9084 - val_loss: 81938899927.0400\n",
      "Epoch 27/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 77409837128.8178 - val_loss: 80284049276.9280\n",
      "Epoch 28/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 75907892911.2178 - val_loss: 78857053732.8640\n",
      "Epoch 29/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 74593363758.6489 - val_loss: 77589748056.0640\n",
      "Epoch 30/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 73434448862.3218 - val_loss: 76440567676.9280\n",
      "Epoch 31/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 72388591382.9831 - val_loss: 75392780075.0080\n",
      "Epoch 32/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 71415718102.8124 - val_loss: 74433648132.0960\n",
      "Epoch 33/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 70519953901.3404 - val_loss: 73519545221.1200\n",
      "Epoch 34/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 69664263299.9822 - val_loss: 72658094981.1200\n",
      "Epoch 35/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 68847307299.9538 - val_loss: 71832857411.5840\n",
      "Epoch 36/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 68061245123.2427 - val_loss: 71055667036.1600\n",
      "Epoch 37/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 67297847898.5671 - val_loss: 70295621206.0160\n",
      "Epoch 38/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 66548314690.9013 - val_loss: 69544804155.3920\n",
      "Epoch 39/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 65802287433.5004 - val_loss: 68800373522.4320\n",
      "Epoch 40/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 65073398211.4702 - val_loss: 68096687341.5680\n",
      "Epoch 41/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 64344636143.8436 - val_loss: 67376283156.4800\n",
      "Epoch 42/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 63622377439.2320 - val_loss: 66666608525.3120\n",
      "Epoch 43/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 62916118846.5778 - val_loss: 65980831694.8480\n",
      "Epoch 44/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 62203820987.7333 - val_loss: 65282057994.2400\n",
      "Epoch 45/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 61489272316.8142 - val_loss: 64595847544.8320\n",
      "Epoch 46/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 60776852852.2809 - val_loss: 63896474681.3440\n",
      "Epoch 47/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 60063808156.1031 - val_loss: 63218145755.1360\n",
      "Epoch 48/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 59352234663.0258 - val_loss: 62518026076.1600\n",
      "Epoch 49/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 58663435479.7227 - val_loss: 61840687693.8240\n",
      "Epoch 50/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 57947576693.1911 - val_loss: 61149969186.8160\n",
      "Epoch 51/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 57228263868.1884 - val_loss: 60470712958.9760\n",
      "Epoch 52/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 56526551886.5067 - val_loss: 59785181331.4560\n",
      "Epoch 53/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 55830205206.0729 - val_loss: 59109734776.8320\n",
      "Epoch 54/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 55112070769.3227 - val_loss: 58437222498.3040\n",
      "Epoch 55/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 54415164367.7582 - val_loss: 57758560747.5200\n",
      "Epoch 56/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 53715308016.0711 - val_loss: 57083334983.6800\n",
      "Epoch 57/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 53020278285.1982 - val_loss: 56407620288.5120\n",
      "Epoch 58/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 52327839163.2782 - val_loss: 55744691372.0320\n",
      "Epoch 59/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 51637411170.0764 - val_loss: 55092800323.5840\n",
      "Epoch 60/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 50955147171.1573 - val_loss: 54431102533.6320\n",
      "Epoch 61/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 50270661181.4400 - val_loss: 53798340689.9200\n",
      "Epoch 62/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 49606753452.0320 - val_loss: 53170362187.7760\n",
      "Epoch 63/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 48940280002.7876 - val_loss: 52562962939.9040\n",
      "Epoch 64/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 48294432174.5351 - val_loss: 51942451609.6000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 47657568632.8320 - val_loss: 51346588499.9680\n",
      "Epoch 66/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 47021006868.0249 - val_loss: 50762424352.7680\n",
      "Epoch 67/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 46416208667.5342 - val_loss: 50203192000.5120\n",
      "Epoch 68/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 45808373923.8400 - val_loss: 49657908953.0880\n",
      "Epoch 69/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 45224969693.8667 - val_loss: 49126869663.7440\n",
      "Epoch 70/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 44664304427.9182 - val_loss: 48596182695.9360\n",
      "Epoch 71/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 44094973529.6569 - val_loss: 48087944560.6400\n",
      "Epoch 72/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 43552793566.3218 - val_loss: 47608031674.3680\n",
      "Epoch 73/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 43020494926.2791 - val_loss: 47138624569.3440\n",
      "Epoch 74/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 42508558965.8738 - val_loss: 46684905472.0000\n",
      "Epoch 75/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 42019984019.0009 - val_loss: 46261620703.2320\n",
      "Epoch 76/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 41545842127.3031 - val_loss: 45843599327.2320\n",
      "Epoch 77/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 41095570467.4987 - val_loss: 45457206345.7280\n",
      "Epoch 78/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 40684484755.4560 - val_loss: 45081749291.0080\n",
      "Epoch 79/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 40268589017.7707 - val_loss: 44748279742.4640\n",
      "Epoch 80/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 39896003919.8720 - val_loss: 44415500320.7680\n",
      "Epoch 81/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 39539986351.9004 - val_loss: 44102965264.3840\n",
      "Epoch 82/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 39227574929.1804 - val_loss: 43853748109.3120\n",
      "Epoch 83/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 38923182232.9173 - val_loss: 43629532086.2720\n",
      "Epoch 84/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 38661943985.9484 - val_loss: 43392752680.9600\n",
      "Epoch 85/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 38413808038.3431 - val_loss: 43201336737.7920\n",
      "Epoch 86/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 38185170548.9636 - val_loss: 43035026391.0400\n",
      "Epoch 87/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 37985103577.0880 - val_loss: 42880589070.3360\n",
      "Epoch 88/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 37803805633.1947 - val_loss: 42742873161.7280\n",
      "Epoch 89/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 37638819459.5271 - val_loss: 42609488461.8240\n",
      "Epoch 90/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 37482808450.1618 - val_loss: 42469810274.3040\n",
      "Epoch 91/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 37333629470.4924 - val_loss: 42362726547.4560\n",
      "Epoch 92/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 37199968257.8204 - val_loss: 42229320515.5840\n",
      "Epoch 93/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 37069577937.8062 - val_loss: 42143725387.7760\n",
      "Epoch 94/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 36951783052.6293 - val_loss: 42038005563.3920\n",
      "Epoch 95/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 36828121777.9484 - val_loss: 41932856033.2800\n",
      "Epoch 96/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 36723432620.0320 - val_loss: 41831469187.0720\n",
      "Epoch 97/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 36608520676.2382 - val_loss: 41729522696.1920\n",
      "Epoch 98/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 36499916601.5716 - val_loss: 41659628158.9760\n",
      "Epoch 99/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 36403063597.7387 - val_loss: 41562989068.2880\n",
      "Epoch 100/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 36300390869.6747 - val_loss: 41495425187.8400\n",
      "Epoch 101/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 36210296844.7431 - val_loss: 41405987717.1200\n",
      "Epoch 102/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 36111330428.7004 - val_loss: 41324723372.0320\n",
      "Epoch 103/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 36024506737.5502 - val_loss: 41244303130.6240\n",
      "Epoch 104/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 35943596851.2000 - val_loss: 41145051512.8320\n",
      "Epoch 105/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 35854970403.0436 - val_loss: 41102605451.2640\n",
      "Epoch 106/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 35773555290.5671 - val_loss: 40970067804.1600\n",
      "Epoch 107/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 35689569865.2729 - val_loss: 40908265390.0800\n",
      "Epoch 108/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 35620038179.0436 - val_loss: 40839950139.3920\n",
      "Epoch 109/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 35535158123.6338 - val_loss: 40770361556.9920\n",
      "Epoch 110/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 35468901096.5618 - val_loss: 40727457923.0720\n",
      "Epoch 111/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 35394392642.9013 - val_loss: 40623544500.2240\n",
      "Epoch 112/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 35327231543.0684 - val_loss: 40549075910.6560\n",
      "Epoch 113/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 35270182793.6711 - val_loss: 40520868528.1280\n",
      "Epoch 114/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 35198319620.5511 - val_loss: 40451491561.4720\n",
      "Epoch 115/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 35132314419.2000 - val_loss: 40329751101.4400\n",
      "Epoch 116/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 35063979320.2062 - val_loss: 40297522724.8640\n",
      "Epoch 117/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 35004962650.3396 - val_loss: 40213783871.4880\n",
      "Epoch 118/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 34941482802.2898 - val_loss: 40160618020.8640\n",
      "Epoch 119/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 34883591010.5316 - val_loss: 40100712054.7840\n",
      "Epoch 120/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 34850501361.6640 - val_loss: 40093808001.0240\n",
      "Epoch 121/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 34772823539.7120 - val_loss: 39996513320.9600\n",
      "Epoch 122/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 34730081830.6844 - val_loss: 39941259591.6800\n",
      "Epoch 123/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 34670426137.4862 - val_loss: 39863061118.9760\n",
      "Epoch 124/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 34613461072.0996 - val_loss: 39818423271.4240\n",
      "Epoch 125/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 34569238446.0800 - val_loss: 39769928892.4160\n",
      "Epoch 126/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 34508189262.7342 - val_loss: 39722162061.3120\n",
      "Epoch 127/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 34463926124.5440 - val_loss: 39694054916.0960\n",
      "Epoch 128/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 34421547353.8844 - val_loss: 39613031055.3600\n",
      "Epoch 129/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 0s 10us/step - loss: 34367007125.9591 - val_loss: 39569588060.1600\n",
      "Epoch 130/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 34323841872.3271 - val_loss: 39494117163.0080\n",
      "Epoch 131/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 34272800524.0604 - val_loss: 39461744836.6080\n",
      "Epoch 132/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 34229787495.9929 - val_loss: 39433501212.6720\n",
      "Epoch 133/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 34190131271.9076 - val_loss: 39354671890.4320\n",
      "Epoch 134/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 34145208580.3236 - val_loss: 39342513815.5520\n",
      "Epoch 135/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 34111710323.5982 - val_loss: 39268700422.1440\n",
      "Epoch 136/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 34060546237.3262 - val_loss: 39246893449.2160\n",
      "Epoch 137/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 34020821485.3404 - val_loss: 39210969628.6720\n",
      "Epoch 138/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33979498668.9422 - val_loss: 39155031408.6400\n",
      "Epoch 139/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33940960983.2676 - val_loss: 39097670107.1360\n",
      "Epoch 140/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33908089330.8018 - val_loss: 39046541672.4480\n",
      "Epoch 141/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33863046587.2782 - val_loss: 39022873444.3520\n",
      "Epoch 142/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 33824089307.3636 - val_loss: 39014701727.7440\n",
      "Epoch 143/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 33791201942.6418 - val_loss: 38963708788.7360\n",
      "Epoch 144/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33757123198.9760 - val_loss: 38937545211.9040\n",
      "Epoch 145/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 33713385950.7769 - val_loss: 38879068454.9120\n",
      "Epoch 146/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33683726055.6516 - val_loss: 38853848236.0320\n",
      "Epoch 147/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33641912995.3849 - val_loss: 38818009415.6800\n",
      "Epoch 148/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 33606459799.7796 - val_loss: 38775335911.4240\n",
      "Epoch 149/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 33574235147.8329 - val_loss: 38705595973.6320\n",
      "Epoch 150/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 33568950080.8533 - val_loss: 38719818858.4960\n",
      "Epoch 151/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 33509609495.6658 - val_loss: 38658284126.2080\n",
      "Epoch 152/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33481226365.6107 - val_loss: 38633392078.8480\n",
      "Epoch 153/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 33451056445.6676 - val_loss: 38617715572.7360\n",
      "Epoch 154/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 33412784257.2516 - val_loss: 38542362607.6160\n",
      "Epoch 155/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 33391291618.6453 - val_loss: 38524643672.0640\n",
      "Epoch 156/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 33356268208.1280 - val_loss: 38490908393.4720\n",
      "Epoch 157/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33327413271.6658 - val_loss: 38487985618.9440\n",
      "Epoch 158/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 33295064563.7120 - val_loss: 38443831197.6960\n",
      "Epoch 159/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 33273390568.7893 - val_loss: 38395919073.2800\n",
      "Epoch 160/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 33252976332.3449 - val_loss: 38372436377.6000\n",
      "Epoch 161/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 33209258602.9511 - val_loss: 38365567221.7600\n",
      "Epoch 162/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 33188798353.8631 - val_loss: 38342951927.8080\n",
      "Epoch 163/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33158836657.2658 - val_loss: 38325038841.8560\n",
      "Epoch 164/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33133754571.8898 - val_loss: 38288769155.0720\n",
      "Epoch 165/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 33103101986.5884 - val_loss: 38253542768.6400\n",
      "Epoch 166/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33079832104.5049 - val_loss: 38233975554.0480\n",
      "Epoch 167/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 33051364374.7556 - val_loss: 38214928367.6160\n",
      "Epoch 168/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33032639883.9467 - val_loss: 38187237474.3040\n",
      "Epoch 169/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33006250495.5449 - val_loss: 38155766956.0320\n",
      "Epoch 170/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 32980731683.7262 - val_loss: 38156243238.9120\n",
      "Epoch 171/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 32965322167.6373 - val_loss: 38133432320.0000\n",
      "Epoch 172/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 32935263212.8853 - val_loss: 38090556669.9520\n",
      "Epoch 173/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 32914447404.6009 - val_loss: 38111656935.4240\n",
      "Epoch 174/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 32891783579.4204 - val_loss: 38048616611.8400\n",
      "Epoch 175/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 32866666181.9733 - val_loss: 37999818604.5440\n",
      "Epoch 176/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 32846815734.4427 - val_loss: 38006610558.9760\n",
      "Epoch 177/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 32835401660.6436 - val_loss: 37982206427.1360\n",
      "Epoch 178/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 32808089503.5164 - val_loss: 37966182219.7760\n",
      "Epoch 179/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 32782791440.6116 - val_loss: 37937569923.0720\n",
      "Epoch 180/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 32763200620.3164 - val_loss: 37911522443.2640\n",
      "Epoch 181/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 32745086535.4524 - val_loss: 37906813222.9120\n",
      "Epoch 182/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 32717479972.4089 - val_loss: 37885059727.3600\n",
      "Epoch 183/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 32710052781.1698 - val_loss: 37834757832.7040\n",
      "Epoch 184/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 32682517379.2996 - val_loss: 37842595151.8720\n",
      "Epoch 185/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 32655705913.5716 - val_loss: 37810801573.8880\n",
      "Epoch 186/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 32641985864.5902 - val_loss: 37793916452.8640\n",
      "Epoch 187/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 32620255885.5396 - val_loss: 37763837526.0160\n",
      "Epoch 188/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 32603543371.7760 - val_loss: 37751537860.6080\n",
      "Epoch 189/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 32584094842.8800 - val_loss: 37746172395.5200\n",
      "Epoch 190/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 32564482659.6693 - val_loss: 37697729658.8800\n",
      "Epoch 191/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 32556844387.8969 - val_loss: 37680019636.2240\n",
      "Epoch 192/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 32526802026.4960 - val_loss: 37659728773.1200\n",
      "Epoch 193/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 0s 11us/step - loss: 32510420764.4444 - val_loss: 37674415620.0960\n",
      "Epoch 194/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 32487788407.4667 - val_loss: 37644449415.1680\n",
      "Epoch 195/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 32481990339.2427 - val_loss: 37624811814.9120\n",
      "Epoch 196/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 32462005141.5040 - val_loss: 37613180289.0240\n",
      "Epoch 197/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 32434488366.4213 - val_loss: 37617174872.0640\n",
      "Epoch 198/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 32415562995.0293 - val_loss: 37598212128.7680\n",
      "Epoch 199/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 32399816299.8613 - val_loss: 37554884968.4480\n",
      "Epoch 200/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 32376786528.9387 - val_loss: 37537354153.9840\n",
      "0.6712296263560307\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = nn_model()\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "history = model.fit(selected_feature_train, price_train,\n",
    "            epochs=200, batch_size=128,\n",
    "            validation_data=(selected_feature_val, price_val))\n",
    "model_score = score(model.predict(selected_feature_val), price_val)\n",
    "print(model_score)\n",
    "\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the future, consider standardizing outputs as well\n",
    "\n",
    "### Regularize:\n",
    "* Heavily parameterized models like neural networks are prone to overfitting\n",
    "* Popular off-the-shelf tools exist to regularize models and prevent overfitting:\n",
    "    - L2 regularization (weight decay)\n",
    "    - Dropout\n",
    "    - Batch normalization\n",
    "    \n",
    "#### These tools come as standard Keras/TF layers!\n",
    "`model.add(keras.layers.Dropout(rate)`\n",
    "`model.add(keras.layers.ActivityRegularization(l1=0.0, l2=0.0)`\n",
    "`model.add(keras.layers.BatchNormalization())`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping and model checkpointing:\n",
    "#### It's unlikely the last iteration is the best, and who knows how long until the thing is converged. Just grab the best validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/400\n",
      "18000/18000 [==============================] - 1s 33us/step - loss: 416651544421.2622 - val_loss: 456259511517.1840\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 456259511517.18402, saving model to best_model.h5\n",
      "Epoch 2/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416275351615.7155 - val_loss: 455326677860.3519\n",
      "\n",
      "Epoch 00002: val_loss improved from 456259511517.18402 to 455326677860.35199, saving model to best_model.h5\n",
      "Epoch 3/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 414689964880.3271 - val_loss: 452503833411.5840\n",
      "\n",
      "Epoch 00003: val_loss improved from 455326677860.35199 to 452503833411.58398, saving model to best_model.h5\n",
      "Epoch 4/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 411070708609.4791 - val_loss: 446986228334.5920\n",
      "\n",
      "Epoch 00004: val_loss improved from 452503833411.58398 to 446986228334.59198, saving model to best_model.h5\n",
      "Epoch 5/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 404691041299.1147 - val_loss: 438088345845.7600\n",
      "\n",
      "Epoch 00005: val_loss improved from 446986228334.59198 to 438088345845.76001, saving model to best_model.h5\n",
      "Epoch 6/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 395193707134.9760 - val_loss: 425492854865.9200\n",
      "\n",
      "Epoch 00006: val_loss improved from 438088345845.76001 to 425492854865.91998, saving model to best_model.h5\n",
      "Epoch 7/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 382342614191.6729 - val_loss: 409169559552.0000\n",
      "\n",
      "Epoch 00007: val_loss improved from 425492854865.91998 to 409169559552.00000, saving model to best_model.h5\n",
      "Epoch 8/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 366015355493.4897 - val_loss: 388959393808.3840\n",
      "\n",
      "Epoch 00008: val_loss improved from 409169559552.00000 to 388959393808.38397, saving model to best_model.h5\n",
      "Epoch 9/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 346449591664.6400 - val_loss: 365161711403.0080\n",
      "\n",
      "Epoch 00009: val_loss improved from 388959393808.38397 to 365161711403.00800, saving model to best_model.h5\n",
      "Epoch 10/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 324079842798.2507 - val_loss: 338854563807.2320\n",
      "\n",
      "Epoch 00010: val_loss improved from 365161711403.00800 to 338854563807.23199, saving model to best_model.h5\n",
      "Epoch 11/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 299622456215.3245 - val_loss: 310327667064.8320\n",
      "\n",
      "Epoch 00011: val_loss improved from 338854563807.23199 to 310327667064.83197, saving model to best_model.h5\n",
      "Epoch 12/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 273810197810.7449 - val_loss: 281015086219.2640\n",
      "\n",
      "Epoch 00012: val_loss improved from 310327667064.83197 to 281015086219.26398, saving model to best_model.h5\n",
      "Epoch 13/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 247658309455.4169 - val_loss: 251876329390.0800\n",
      "\n",
      "Epoch 00013: val_loss improved from 281015086219.26398 to 251876329390.07999, saving model to best_model.h5\n",
      "Epoch 14/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 221969782145.0240 - val_loss: 223936291012.6080\n",
      "\n",
      "Epoch 00014: val_loss improved from 251876329390.07999 to 223936291012.60800, saving model to best_model.h5\n",
      "Epoch 15/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 197645427277.8240 - val_loss: 197799311704.0640\n",
      "\n",
      "Epoch 00015: val_loss improved from 223936291012.60800 to 197799311704.06400, saving model to best_model.h5\n",
      "Epoch 16/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 175316568695.6942 - val_loss: 174281057501.1840\n",
      "\n",
      "Epoch 00016: val_loss improved from 197799311704.06400 to 174281057501.18399, saving model to best_model.h5\n",
      "Epoch 17/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 155461359821.7102 - val_loss: 153989718999.0400\n",
      "\n",
      "Epoch 00017: val_loss improved from 174281057501.18399 to 153989718999.04001, saving model to best_model.h5\n",
      "Epoch 18/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 138388227504.3556 - val_loss: 136791897604.0960\n",
      "\n",
      "Epoch 00018: val_loss improved from 153989718999.04001 to 136791897604.09599, saving model to best_model.h5\n",
      "Epoch 19/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 124003460943.4169 - val_loss: 122836608352.2560\n",
      "\n",
      "Epoch 00019: val_loss improved from 136791897604.09599 to 122836608352.25600, saving model to best_model.h5\n",
      "Epoch 20/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 112310590883.6124 - val_loss: 111870702845.9520\n",
      "\n",
      "Epoch 00020: val_loss improved from 122836608352.25600 to 111870702845.95200, saving model to best_model.h5\n",
      "Epoch 21/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 103110011839.3742 - val_loss: 103343601876.9920\n",
      "\n",
      "Epoch 00021: val_loss improved from 111870702845.95200 to 103343601876.99200, saving model to best_model.h5\n",
      "Epoch 22/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 95954258588.1031 - val_loss: 96986140246.0160\n",
      "\n",
      "Epoch 00022: val_loss improved from 103343601876.99200 to 96986140246.01601, saving model to best_model.h5\n",
      "Epoch 23/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 90434397033.8133 - val_loss: 92076851200.0000\n",
      "\n",
      "Epoch 00023: val_loss improved from 96986140246.01601 to 92076851200.00000, saving model to best_model.h5\n",
      "Epoch 24/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 86121230106.6240 - val_loss: 88317148004.3520\n",
      "\n",
      "Epoch 00024: val_loss improved from 92076851200.00000 to 88317148004.35201, saving model to best_model.h5\n",
      "Epoch 25/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 82706072520.4764 - val_loss: 85273224019.9680\n",
      "\n",
      "Epoch 00025: val_loss improved from 88317148004.35201 to 85273224019.96800, saving model to best_model.h5\n",
      "Epoch 26/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 79954325493.0773 - val_loss: 82783483854.8480\n",
      "\n",
      "Epoch 00026: val_loss improved from 85273224019.96800 to 82783483854.84801, saving model to best_model.h5\n",
      "Epoch 27/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 77693653308.7573 - val_loss: 80722644697.0880\n",
      "\n",
      "Epoch 00027: val_loss improved from 82783483854.84801 to 80722644697.08800, saving model to best_model.h5\n",
      "Epoch 28/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 75807112531.5129 - val_loss: 78950488080.3840\n",
      "\n",
      "Epoch 00028: val_loss improved from 80722644697.08800 to 78950488080.38400, saving model to best_model.h5\n",
      "Epoch 29/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 74205346216.1635 - val_loss: 77438394302.4640\n",
      "\n",
      "Epoch 00029: val_loss improved from 78950488080.38400 to 77438394302.46400, saving model to best_model.h5\n",
      "Epoch 30/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 72839366013.3831 - val_loss: 76100882530.3040\n",
      "\n",
      "Epoch 00030: val_loss improved from 77438394302.46400 to 76100882530.30400, saving model to best_model.h5\n",
      "Epoch 31/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 71630325263.0187 - val_loss: 74927345205.2480\n",
      "\n",
      "Epoch 00031: val_loss improved from 76100882530.30400 to 74927345205.24800, saving model to best_model.h5\n",
      "Epoch 32/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 70559261884.4160 - val_loss: 73853330030.5920\n",
      "\n",
      "Epoch 00032: val_loss improved from 74927345205.24800 to 73853330030.59200, saving model to best_model.h5\n",
      "Epoch 33/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 69578067993.4862 - val_loss: 72866725036.0320\n",
      "\n",
      "Epoch 00033: val_loss improved from 73853330030.59200 to 72866725036.03200, saving model to best_model.h5\n",
      "Epoch 34/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 68685977328.7538 - val_loss: 71949478526.9760\n",
      "\n",
      "Epoch 00034: val_loss improved from 72866725036.03200 to 71949478526.97600, saving model to best_model.h5\n",
      "Epoch 35/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 67829586387.8542 - val_loss: 71087979429.8880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00035: val_loss improved from 71949478526.97600 to 71087979429.88800, saving model to best_model.h5\n",
      "Epoch 36/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 67020454435.9538 - val_loss: 70269258629.1200\n",
      "\n",
      "Epoch 00036: val_loss improved from 71087979429.88800 to 70269258629.12000, saving model to best_model.h5\n",
      "Epoch 37/400\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 66249264176.2418 - val_loss: 69490233540.6080\n",
      "\n",
      "Epoch 00037: val_loss improved from 70269258629.12000 to 69490233540.60800, saving model to best_model.h5\n",
      "Epoch 38/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 65487839723.5200 - val_loss: 68727250124.8000\n",
      "\n",
      "Epoch 00038: val_loss improved from 69490233540.60800 to 68727250124.80000, saving model to best_model.h5\n",
      "Epoch 39/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 64752765173.7600 - val_loss: 67985605165.0560\n",
      "\n",
      "Epoch 00039: val_loss improved from 68727250124.80000 to 67985605165.05600, saving model to best_model.h5\n",
      "Epoch 40/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 64032211613.9236 - val_loss: 67269534089.2160\n",
      "\n",
      "Epoch 00040: val_loss improved from 67985605165.05600 to 67269534089.21600, saving model to best_model.h5\n",
      "Epoch 41/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 63311447909.2622 - val_loss: 66559629852.6720\n",
      "\n",
      "Epoch 00041: val_loss improved from 67269534089.21600 to 66559629852.67200, saving model to best_model.h5\n",
      "Epoch 42/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 62604334745.3724 - val_loss: 65846278914.0480\n",
      "\n",
      "Epoch 00042: val_loss improved from 66559629852.67200 to 65846278914.04800, saving model to best_model.h5\n",
      "Epoch 43/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 61906665504.7680 - val_loss: 65135463825.4080\n",
      "\n",
      "Epoch 00043: val_loss improved from 65846278914.04800 to 65135463825.40800, saving model to best_model.h5\n",
      "Epoch 44/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 61208331426.9298 - val_loss: 64449455652.8640\n",
      "\n",
      "Epoch 00044: val_loss improved from 65135463825.40800 to 64449455652.86400, saving model to best_model.h5\n",
      "Epoch 45/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 60514366994.6596 - val_loss: 63770025197.5680\n",
      "\n",
      "Epoch 00045: val_loss improved from 64449455652.86400 to 63770025197.56800, saving model to best_model.h5\n",
      "Epoch 46/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 59825851749.7173 - val_loss: 63082007691.2640\n",
      "\n",
      "Epoch 00046: val_loss improved from 63770025197.56800 to 63082007691.26400, saving model to best_model.h5\n",
      "Epoch 47/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 59139245417.3582 - val_loss: 62419569410.0480\n",
      "\n",
      "Epoch 00047: val_loss improved from 63082007691.26400 to 62419569410.04800, saving model to best_model.h5\n",
      "Epoch 48/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 58456129885.5253 - val_loss: 61734444597.2480\n",
      "\n",
      "Epoch 00048: val_loss improved from 62419569410.04800 to 61734444597.24800, saving model to best_model.h5\n",
      "Epoch 49/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 57767254057.8702 - val_loss: 61078274899.9680\n",
      "\n",
      "Epoch 00049: val_loss improved from 61734444597.24800 to 61078274899.96800, saving model to best_model.h5\n",
      "Epoch 50/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 57085206207.6018 - val_loss: 60410257309.6960\n",
      "\n",
      "Epoch 00050: val_loss improved from 61078274899.96800 to 60410257309.69600, saving model to best_model.h5\n",
      "Epoch 51/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 56398150340.1529 - val_loss: 59743652872.1920\n",
      "\n",
      "Epoch 00051: val_loss improved from 60410257309.69600 to 59743652872.19200, saving model to best_model.h5\n",
      "Epoch 52/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 55723173846.1298 - val_loss: 59066802044.9280\n",
      "\n",
      "Epoch 00052: val_loss improved from 59743652872.19200 to 59066802044.92800, saving model to best_model.h5\n",
      "Epoch 53/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 55051055398.9120 - val_loss: 58403682713.6000\n",
      "\n",
      "Epoch 00053: val_loss improved from 59066802044.92800 to 58403682713.60000, saving model to best_model.h5\n",
      "Epoch 54/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 54361073836.9422 - val_loss: 57736142684.1600\n",
      "\n",
      "Epoch 00054: val_loss improved from 58403682713.60000 to 57736142684.16000, saving model to best_model.h5\n",
      "Epoch 55/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 53684298714.6809 - val_loss: 57090447507.4560\n",
      "\n",
      "Epoch 00055: val_loss improved from 57736142684.16000 to 57090447507.45600, saving model to best_model.h5\n",
      "Epoch 56/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 53011764186.6809 - val_loss: 56441686458.3680\n",
      "\n",
      "Epoch 00056: val_loss improved from 57090447507.45600 to 56441686458.36800, saving model to best_model.h5\n",
      "Epoch 57/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 52344646442.0978 - val_loss: 55805472899.0720\n",
      "\n",
      "Epoch 00057: val_loss improved from 56441686458.36800 to 55805472899.07200, saving model to best_model.h5\n",
      "Epoch 58/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 51682371636.7929 - val_loss: 55169162608.6400\n",
      "\n",
      "Epoch 00058: val_loss improved from 55805472899.07200 to 55169162608.64000, saving model to best_model.h5\n",
      "Epoch 59/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 51022363259.3351 - val_loss: 54536761311.2320\n",
      "\n",
      "Epoch 00059: val_loss improved from 55169162608.64000 to 54536761311.23200, saving model to best_model.h5\n",
      "Epoch 60/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 50374866119.3387 - val_loss: 53918748966.9120\n",
      "\n",
      "Epoch 00060: val_loss improved from 54536761311.23200 to 53918748966.91200, saving model to best_model.h5\n",
      "Epoch 61/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 49730345100.1742 - val_loss: 53305510166.5280\n",
      "\n",
      "Epoch 00061: val_loss improved from 53918748966.91200 to 53305510166.52800, saving model to best_model.h5\n",
      "Epoch 62/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 49080986053.2907 - val_loss: 52701526425.6000\n",
      "\n",
      "Epoch 00062: val_loss improved from 53305510166.52800 to 52701526425.60000, saving model to best_model.h5\n",
      "Epoch 63/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 48452242960.8391 - val_loss: 52115588546.5600\n",
      "\n",
      "Epoch 00063: val_loss improved from 52701526425.60000 to 52115588546.56000, saving model to best_model.h5\n",
      "Epoch 64/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 47825720188.0178 - val_loss: 51537091952.6400\n",
      "\n",
      "Epoch 00064: val_loss improved from 52115588546.56000 to 51537091952.64000, saving model to best_model.h5\n",
      "Epoch 65/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 47210270625.3369 - val_loss: 50966590816.2560\n",
      "\n",
      "Epoch 00065: val_loss improved from 51537091952.64000 to 50966590816.25600, saving model to best_model.h5\n",
      "Epoch 66/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 46606872834.5031 - val_loss: 50407593246.7200\n",
      "\n",
      "Epoch 00066: val_loss improved from 50966590816.25600 to 50407593246.72000, saving model to best_model.h5\n",
      "Epoch 67/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 46011090694.5991 - val_loss: 49850877476.8640\n",
      "\n",
      "Epoch 00067: val_loss improved from 50407593246.72000 to 49850877476.86400, saving model to best_model.h5\n",
      "Epoch 68/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 45432101229.9093 - val_loss: 49312805748.7360\n",
      "\n",
      "Epoch 00068: val_loss improved from 49850877476.86400 to 49312805748.73600, saving model to best_model.h5\n",
      "Epoch 69/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 44856975807.8293 - val_loss: 48810304962.5600\n",
      "\n",
      "Epoch 00069: val_loss improved from 49312805748.73600 to 48810304962.56000, saving model to best_model.h5\n",
      "Epoch 70/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 44300318863.8151 - val_loss: 48290853781.5040\n",
      "\n",
      "Epoch 00070: val_loss improved from 48810304962.56000 to 48290853781.50400, saving model to best_model.h5\n",
      "Epoch 71/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 0s 10us/step - loss: 43743356263.5378 - val_loss: 47804756328.4480\n",
      "\n",
      "Epoch 00071: val_loss improved from 48290853781.50400 to 47804756328.44800, saving model to best_model.h5\n",
      "Epoch 72/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 43205840306.1760 - val_loss: 47318267658.2400\n",
      "\n",
      "Epoch 00072: val_loss improved from 47804756328.44800 to 47318267658.24000, saving model to best_model.h5\n",
      "Epoch 73/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 42700101029.4329 - val_loss: 46844257730.5600\n",
      "\n",
      "Epoch 00073: val_loss improved from 47318267658.24000 to 46844257730.56000, saving model to best_model.h5\n",
      "Epoch 74/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 42188321698.2471 - val_loss: 46398046797.8240\n",
      "\n",
      "Epoch 00074: val_loss improved from 46844257730.56000 to 46398046797.82400, saving model to best_model.h5\n",
      "Epoch 75/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 41711528662.3573 - val_loss: 45974937632.7680\n",
      "\n",
      "Epoch 00075: val_loss improved from 46398046797.82400 to 45974937632.76800, saving model to best_model.h5\n",
      "Epoch 76/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 41230647192.2347 - val_loss: 45567350505.4720\n",
      "\n",
      "Epoch 00076: val_loss improved from 45974937632.76800 to 45567350505.47200, saving model to best_model.h5\n",
      "Epoch 77/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 40789517465.8276 - val_loss: 45184124059.6480\n",
      "\n",
      "Epoch 00077: val_loss improved from 45567350505.47200 to 45184124059.64800, saving model to best_model.h5\n",
      "Epoch 78/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 40363136960.2844 - val_loss: 44806291161.0880\n",
      "\n",
      "Epoch 00078: val_loss improved from 45184124059.64800 to 44806291161.08800, saving model to best_model.h5\n",
      "Epoch 79/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 39956005410.1333 - val_loss: 44462505394.1760\n",
      "\n",
      "Epoch 00079: val_loss improved from 44806291161.08800 to 44462505394.17600, saving model to best_model.h5\n",
      "Epoch 80/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 39568717312.4551 - val_loss: 44135945011.2000\n",
      "\n",
      "Epoch 00080: val_loss improved from 44462505394.17600 to 44135945011.20000, saving model to best_model.h5\n",
      "Epoch 81/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 39207667118.5351 - val_loss: 43842293596.1600\n",
      "\n",
      "Epoch 00081: val_loss improved from 44135945011.20000 to 43842293596.16000, saving model to best_model.h5\n",
      "Epoch 82/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 38874836118.1867 - val_loss: 43573665923.0720\n",
      "\n",
      "Epoch 00082: val_loss improved from 43842293596.16000 to 43573665923.07200, saving model to best_model.h5\n",
      "Epoch 83/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 38556568292.9209 - val_loss: 43324862824.4480\n",
      "\n",
      "Epoch 00083: val_loss improved from 43573665923.07200 to 43324862824.44800, saving model to best_model.h5\n",
      "Epoch 84/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 38283844718.1369 - val_loss: 43086194343.9360\n",
      "\n",
      "Epoch 00084: val_loss improved from 43324862824.44800 to 43086194343.93600, saving model to best_model.h5\n",
      "Epoch 85/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 38014050810.0835 - val_loss: 42900323696.6400\n",
      "\n",
      "Epoch 00085: val_loss improved from 43086194343.93600 to 42900323696.64000, saving model to best_model.h5\n",
      "Epoch 86/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 37770691415.6089 - val_loss: 42688227016.7040\n",
      "\n",
      "Epoch 00086: val_loss improved from 42900323696.64000 to 42688227016.70400, saving model to best_model.h5\n",
      "Epoch 87/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 37545718502.7413 - val_loss: 42516973158.4000\n",
      "\n",
      "Epoch 00087: val_loss improved from 42688227016.70400 to 42516973158.40000, saving model to best_model.h5\n",
      "Epoch 88/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 37331203635.4276 - val_loss: 42372992991.2320\n",
      "\n",
      "Epoch 00088: val_loss improved from 42516973158.40000 to 42372992991.23200, saving model to best_model.h5\n",
      "Epoch 89/400\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 37159389651.8542 - val_loss: 42236020293.6320\n",
      "\n",
      "Epoch 00089: val_loss improved from 42372992991.23200 to 42236020293.63200, saving model to best_model.h5\n",
      "Epoch 90/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 36972910670.2791 - val_loss: 42106678542.3360\n",
      "\n",
      "Epoch 00090: val_loss improved from 42236020293.63200 to 42106678542.33600, saving model to best_model.h5\n",
      "Epoch 91/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 36824012561.5218 - val_loss: 41988672258.0480\n",
      "\n",
      "Epoch 00091: val_loss improved from 42106678542.33600 to 41988672258.04800, saving model to best_model.h5\n",
      "Epoch 92/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 36681343817.0453 - val_loss: 41857367539.7120\n",
      "\n",
      "Epoch 00092: val_loss improved from 41988672258.04800 to 41857367539.71200, saving model to best_model.h5\n",
      "Epoch 93/400\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 36550880508.1316 - val_loss: 41792017858.5600\n",
      "\n",
      "Epoch 00093: val_loss improved from 41857367539.71200 to 41792017858.56000, saving model to best_model.h5\n",
      "Epoch 94/400\n",
      "12160/18000 [===================>..........] - ETA: 0s - loss: 33966497490.1895"
     ]
    }
   ],
   "source": [
    "# Set callback functions to early stop training and save the \n",
    "# best model so far\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
    "            ModelCheckpoint(filepath='best_model.h5',\n",
    "                            monitor='val_loss',\n",
    "                            save_best_only=True,\n",
    "                           verbose=1)]\n",
    "\n",
    "model = nn_model(layers=[20,20,20])\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "history = model.fit(selected_feature_train, price_train,\n",
    "            epochs=400, callbacks=callbacks, batch_size=128,\n",
    "            validation_data=(selected_feature_val, price_val))\n",
    "\n",
    "model_score = score(model.predict(selected_feature_val), price_val)\n",
    "print(f\"Model score: {model_score}\")\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You don't have to remember these resources because they're here when you need them\n",
    "https://www.tensorflow.org/api_docs\n",
    "\n",
    "https://keras.io/\n",
    "\n",
    "https://www.tensorflow.org/tutorials/\n",
    "\n",
    "https://www.google.com\n",
    "\n",
    "### Don't trust me, trust your validation errors\n",
    "### Don't look at your test set until you're actually going to test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
